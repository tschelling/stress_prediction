{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40bfa29",
   "metadata": {},
   "source": [
    "# Load and select data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83a6a84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "do_winsorize = False # Assuming this might be used later\n",
    "lags = 1\n",
    "reduce_banks = True\n",
    "reduce_banks_to = 20\n",
    "\n",
    "# --- Load Data ---\n",
    "# Suppress warnings temporarily if needed, though fixing them is better\n",
    "# warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "# warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "fred = pd.read_parquet('data/fred/macro_data_processed.parquet')\n",
    "fdic = pd.read_parquet('data/fdic/fdic_data_processed.parquet')\n",
    "yahoo = pd.read_parquet('data/yahoo/yahoo.parquet')\n",
    "\n",
    "data_selected = fdic.merge(fred, on='date', how='left').merge(yahoo, on='date', how='left')\n",
    "data_selected.set_index(['id', 'date'], inplace=True)\n",
    "\n",
    "variables = [\n",
    "    'gdp_qoq', 'cpi_qoq', 'sp500_qoq', 'corp_bond_spread',\n",
    "    'cons_sentiment_qoq', 'unemployment', 'unemployment_diff', 'household_delinq', 'household_delinq_diff',\n",
    "    'vix_qoq', 'spread_10y_3m',\n",
    "    'log_total_assets', 'deposit_ratio', 'loan_to_asset_ratio',\n",
    "    'equity_to_asset_ratio', 'trading_assets_ratio',\n",
    "    'roa'\n",
    "]\n",
    "\n",
    "data_selected = data_selected[variables].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fabb331",
   "metadata": {},
   "source": [
    "## Test the selected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "becc91ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             gdp_qoq        cpi_qoq      sp500_qoq  corp_bond_spread  \\\n",
      "count  642383.000000  642383.000000  642383.000000     642383.000000   \n",
      "mean        0.010699       0.006009       0.016421          0.025601   \n",
      "std         0.013171       0.006112       0.083588          0.007509   \n",
      "min        -0.082485      -0.022900      -0.225582          0.014600   \n",
      "25%         0.007046       0.003699      -0.023018          0.020100   \n",
      "50%         0.011541       0.006524       0.025686          0.025300   \n",
      "75%         0.014300       0.009025       0.070272          0.029900   \n",
      "max         0.087739       0.023894       0.199529          0.055800   \n",
      "\n",
      "       cons_sentiment_qoq   unemployment  unemployment_diff  household_delinq  \\\n",
      "count       642383.000000  642383.000000      642383.000000     642383.000000   \n",
      "mean            -0.001395       0.058664           0.000134          0.035795   \n",
      "std              0.069664       0.018914           0.009757          0.012519   \n",
      "min             -0.232919       0.035000          -0.042000          0.015300   \n",
      "25%             -0.041445       0.045000          -0.002000          0.024800   \n",
      "50%              0.000000       0.054000          -0.001000          0.036500   \n",
      "75%              0.042328       0.068000           0.001000          0.045800   \n",
      "max              0.208012       0.130000           0.092000          0.067700   \n",
      "\n",
      "       household_delinq_diff        vix_qoq  spread_10y_3m  log_total_assets  \\\n",
      "count          642383.000000  642383.000000  642383.000000     644599.000000   \n",
      "mean               -0.000190       0.022962       0.016755         12.097579   \n",
      "std                 0.002408       0.276075       0.012394          1.499797   \n",
      "min                -0.006800      -0.392118      -0.014800        -20.723266   \n",
      "25%                -0.001600      -0.123752       0.007100         11.163155   \n",
      "50%                -0.000300      -0.037736       0.018100         11.949372   \n",
      "75%                 0.001100       0.065760       0.026800         12.847571   \n",
      "max                 0.008700       1.337455       0.036100         21.999775   \n",
      "\n",
      "       deposit_ratio  loan_to_asset_ratio  equity_to_asset_ratio  \\\n",
      "count  630383.000000        644538.000000          368301.000000   \n",
      "mean        0.817050             0.623481               0.122591   \n",
      "std         0.133025             0.180120               0.100036   \n",
      "min         0.000000             0.000000              -2.149533   \n",
      "25%         0.797856             0.530465               0.090008   \n",
      "50%         0.848010             0.654570               0.104607   \n",
      "75%         0.882864             0.751627               0.125337   \n",
      "max         8.829756             5.459194               1.000000   \n",
      "\n",
      "       trading_assets_ratio            roa  \n",
      "count         644538.000000  644538.000000  \n",
      "mean               0.000496       0.002302  \n",
      "std                0.011012       0.068498  \n",
      "min               -0.000126     -50.841121  \n",
      "25%                0.000000       0.001273  \n",
      "50%                0.000000       0.002349  \n",
      "75%                0.000000       0.003437  \n",
      "max                1.273462       1.716426  \n"
     ]
    }
   ],
   "source": [
    "print(data_selected.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafc6e2c",
   "metadata": {},
   "source": [
    "# Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d066634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of rows: 644599\n",
      "Deleted 0 rows with zero or negative total assets.\n",
      "Deleted 96 rows with ROA < -1 or > 1.\n",
      "Deleted 137 rows with loans to total assets ratio >= 1.\n",
      "Deleted 276166 rows with equity to total assets ratio >= 1.\n",
      "Deleted 0 rows with trading assets to total assets ratio < 0 or >= 1.\n",
      "Deleted 300 rows with equity to total assets ratio < 0 or >= 1.\n",
      "\n",
      "Total rows deleted: 276699\n",
      "Remaining rows: 367900\n",
      "\n",
      "Shape after cleaning: (367900, 17)\n"
     ]
    }
   ],
   "source": [
    "data_cleaned = data_selected.copy()\n",
    "initial_rows = len(data_cleaned)\n",
    "cleaning_tracker = {}\n",
    "print(f\"Initial number of rows: {initial_rows}\")\n",
    "\n",
    "# Delete rows with zero total assets\n",
    "rows_before = len(data_cleaned)\n",
    "data_cleaned = data_cleaned[data_cleaned['log_total_assets'] > -np.inf]\n",
    "rows_deleted = rows_before - len(data_cleaned)\n",
    "cleaning_tracker['cleaning_ta_zero'] = rows_deleted\n",
    "print(f\"Deleted {rows_deleted} rows with zero or negative total assets.\")\n",
    "\n",
    "# Delete roa values smaller than -1 and greater than 1\n",
    "rows_before = len(data_cleaned)\n",
    "data_cleaned = data_cleaned[(data_cleaned['roa'] > -1) & (data_cleaned['roa'] < 1)]\n",
    "rows_deleted = rows_before - len(data_cleaned)\n",
    "cleaning_tracker['cleaning_roa_invalid'] = rows_deleted\n",
    "print(f\"Deleted {rows_deleted} rows with ROA < -1 or > 1.\")\n",
    "\n",
    "# Delete rows with loans to total assets ratio greater than or equal to 1\n",
    "rows_before = len(data_cleaned)\n",
    "data_cleaned = data_cleaned[data_cleaned['loan_to_asset_ratio'] < 1]\n",
    "rows_deleted = rows_before - len(data_cleaned)\n",
    "cleaning_tracker['cleaning_l2ta_ge_1'] = rows_deleted\n",
    "print(f\"Deleted {rows_deleted} rows with loans to total assets ratio >= 1.\")\n",
    "\n",
    "# Delete rows with equity to total assets ratio greater than or equal to 1\n",
    "rows_before = len(data_cleaned)\n",
    "data_cleaned = data_cleaned[data_cleaned['equity_to_asset_ratio'] < 1]\n",
    "rows_deleted = rows_before - len(data_cleaned)\n",
    "cleaning_tracker['cleaning_eq2ta_ge_1'] = rows_deleted\n",
    "print(f\"Deleted {rows_deleted} rows with equity to total assets ratio >= 1.\")\n",
    "\n",
    "# Delete rows with trading assets to total assets ratio < 0 or >= 1\n",
    "rows_before = len(data_cleaned)\n",
    "data_cleaned = data_cleaned[(data_cleaned['trading_assets_ratio'] < 1) & (data_cleaned['trading_assets_ratio'] >= 0)]\n",
    "rows_deleted = rows_before - len(data_cleaned)\n",
    "cleaning_tracker['cleaning_trada_invalid'] = rows_deleted\n",
    "print(f\"Deleted {rows_deleted} rows with trading assets to total assets ratio < 0 or >= 1.\")\n",
    "\n",
    "# Delete rows with equity to total assets ratio < 0 or >= 1\n",
    "rows_before = len(data_cleaned)\n",
    "data_cleaned = data_cleaned[(data_cleaned['equity_to_asset_ratio'] < 1) & (data_cleaned['equity_to_asset_ratio'] >= 0)]\n",
    "rows_deleted = rows_before - len(data_cleaned)\n",
    "cleaning_tracker['cleaning_eq2ta_invalid'] = rows_deleted\n",
    "print(f\"Deleted {rows_deleted} rows with equity to total assets ratio < 0 or >= 1.\")\n",
    "\n",
    "\n",
    "\n",
    "total_deleted = sum(cleaning_tracker.values())\n",
    "print(f\"\\nTotal rows deleted: {total_deleted}\")\n",
    "print(f\"Remaining rows: {len(data_cleaned)}\")\n",
    "\n",
    "# Optional: Display the tracker dictionary\n",
    "# print(f\"Cleaning tracker: {cleaning_tracker}\")\n",
    "\n",
    "# Assign the cleaned data back to data_processed for subsequent steps\n",
    "# Note: The next cell (CELL INDEX 6) uses data_processed.\n",
    "# If this cell is intended to be the primary cleaning step,\n",
    "# rename data_cleaned to data_processed here.\n",
    "print(f\"\\nShape after cleaning: {data_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6fac02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             gdp_qoq        cpi_qoq      sp500_qoq  corp_bond_spread  \\\n",
      "count  367889.000000  367889.000000  367889.000000     367889.000000   \n",
      "mean        0.010970       0.005876       0.033971          0.025775   \n",
      "std         0.016178       0.005752       0.078573          0.007155   \n",
      "min        -0.082485      -0.009760      -0.200011          0.014600   \n",
      "25%         0.007737       0.003472       0.004366          0.021000   \n",
      "50%         0.011773       0.005633       0.046941          0.025300   \n",
      "75%         0.014546       0.007998       0.084737          0.029900   \n",
      "max         0.087739       0.023894       0.199529          0.054900   \n",
      "\n",
      "       cons_sentiment_qoq   unemployment  unemployment_diff  household_delinq  \\\n",
      "count       367889.000000  367889.000000      367889.000000     367889.000000   \n",
      "mean             0.006087       0.062219          -0.000360          0.030213   \n",
      "std              0.070819       0.023350           0.012645          0.013137   \n",
      "min             -0.232919       0.035000          -0.042000          0.015300   \n",
      "25%             -0.031088       0.040000          -0.003000          0.022100   \n",
      "50%              0.003286       0.055000          -0.001000          0.025400   \n",
      "75%              0.043924       0.083000           0.000000          0.032000   \n",
      "max              0.208012       0.130000           0.092000          0.067700   \n",
      "\n",
      "       household_delinq_diff        vix_qoq  spread_10y_3m  log_total_assets  \\\n",
      "count          367889.000000  367889.000000  367889.000000     367900.000000   \n",
      "mean               -0.000492       0.000202       0.015711         12.376955   \n",
      "std                 0.002385       0.266349       0.012560          1.466554   \n",
      "min                -0.006800      -0.392118      -0.014800          5.298317   \n",
      "25%                -0.001600      -0.170290       0.007700         11.442052   \n",
      "50%                -0.000400      -0.057587       0.017300         12.215731   \n",
      "75%                 0.000600       0.051383       0.025300         13.106443   \n",
      "max                 0.008700       1.231594       0.036100         21.999775   \n",
      "\n",
      "       deposit_ratio  loan_to_asset_ratio  equity_to_asset_ratio  \\\n",
      "count  367900.000000        367900.000000          367900.000000   \n",
      "mean        0.825611             0.620726               0.122585   \n",
      "std         0.123772             0.179807               0.099423   \n",
      "min         0.000000             0.000000               0.000020   \n",
      "25%         0.806453             0.525334               0.090046   \n",
      "50%         0.852740             0.652545               0.104627   \n",
      "75%         0.884835             0.750683               0.125338   \n",
      "max         0.998614             0.999856               0.999930   \n",
      "\n",
      "       trading_assets_ratio            roa  \n",
      "count         367900.000000  367900.000000  \n",
      "mean               0.000464       0.002370  \n",
      "std                0.010875       0.010535  \n",
      "min                0.000000      -0.737021  \n",
      "25%                0.000000       0.001157  \n",
      "50%                0.000000       0.002199  \n",
      "75%                0.000000       0.003280  \n",
      "max                0.738957       0.950536  \n"
     ]
    }
   ],
   "source": [
    "print(data_cleaned.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad8f95",
   "metadata": {},
   "source": [
    "# Winsorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d75d84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winsorized the following columns: ['log_total_assets', 'deposit_ratio', 'loan_to_asset_ratio', 'equity_to_asset_ratio', 'trading_assets_ratio', 'roa']\n"
     ]
    }
   ],
   "source": [
    "def winsorize_dataframe(df, vars_to_winsorize, do_winsorize=False, lower_percentile=0.02, upper_percentile=0.98):\n",
    "    \"\"\"\n",
    "    Winsorizes specified columns of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        vars_to_winsorize (list): A list of column names to winsorize.\n",
    "        do_winsorize (bool): If True, performs winsorization. Otherwise, returns a copy of the original DataFrame.\n",
    "        lower_percentile (float): The lower percentile for clipping.\n",
    "        upper_percentile (float): The upper percentile for clipping.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The winsorized (or original) DataFrame.\n",
    "    \"\"\"\n",
    "    df_out = df.copy(deep=True)\n",
    "\n",
    "    if not do_winsorize:\n",
    "        return df_out\n",
    "\n",
    "    def winsorize_series(series, lower_p, upper_p):\n",
    "        lower_bound = series.quantile(lower_p)\n",
    "        upper_bound = series.quantile(upper_p)\n",
    "        return np.clip(series, lower_bound, upper_bound)\n",
    "\n",
    "    print(f\"Winsorizing columns at [{lower_percentile}, {upper_percentile}] percentiles...\")\n",
    "    for col in vars_to_winsorize:\n",
    "        if col in df_out.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df_out[col]):\n",
    "                print(f\" - Winsorizing {col}\")\n",
    "                df_out[col] = winsorize_series(df_out[col], lower_percentile, upper_percentile)\n",
    "            else:\n",
    "                print(f\" - Skipping non-numeric column {col}\")\n",
    "        else:\n",
    "            print(f\" - Skipping column {col} (not found in DataFrame)\")\n",
    "    print(\"Winsorization complete.\")\n",
    "    return df_out\n",
    "\n",
    "# --- Apply Winsorization ---\n",
    "\n",
    "\n",
    "# Define the columns to be winsorized in the features DataFrame.\n",
    "# This is a simple heuristic based on column names. Adjust as needed.\n",
    "accounting_columns = [col for col in data_cleaned.columns if 'ratio' in col or 'assets' in col or 'roa' in col]\n",
    "\n",
    "# Winsorize features\n",
    "data_winsorized = winsorize_dataframe(data_cleaned, accounting_columns, do_winsorize=do_winsorize, lower_percentile=0.02, upper_percentile=0.98)\n",
    "\n",
    "print(f\"Winsorized the following columns: {accounting_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98882f75",
   "metadata": {},
   "source": [
    "# Lag the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f6d5900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting lag generation...\n",
      "Generated 18 lagged features.\n",
      "Concatenating lagged features...\n",
      "Concatenation complete.\n",
      "Shape before dropping NaNs: (367900, 36)\n",
      "Nr of banks before dropping NaNs: 8342\n",
      "Shape after dropping NaNs: (359547, 36)\n",
      "Nr of banks after dropping NaNs: 8259\n",
      "Min date: 157.0\n",
      "Max date: 219.0\n",
      "\n",
      "DataFrame Info after processing:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 359547 entries, ('1000052', np.float64(157.0)) to ('999935', np.float64(219.0))\n",
      "Data columns (total 36 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   gdp_qoq                       359547 non-null  float64\n",
      " 1   cpi_qoq                       359547 non-null  float64\n",
      " 2   sp500_qoq                     359547 non-null  float64\n",
      " 3   corp_bond_spread              359547 non-null  float64\n",
      " 4   cons_sentiment_qoq            359547 non-null  float64\n",
      " 5   unemployment                  359547 non-null  float64\n",
      " 6   unemployment_diff             359547 non-null  float64\n",
      " 7   household_delinq              359547 non-null  float64\n",
      " 8   household_delinq_diff         359547 non-null  float64\n",
      " 9   vix_qoq                       359547 non-null  float64\n",
      " 10  spread_10y_3m                 359547 non-null  float64\n",
      " 11  log_total_assets              359547 non-null  float64\n",
      " 12  deposit_ratio                 359547 non-null  float64\n",
      " 13  loan_to_asset_ratio           359547 non-null  float64\n",
      " 14  equity_to_asset_ratio         359547 non-null  float64\n",
      " 15  trading_assets_ratio          359547 non-null  float64\n",
      " 16  roa                           359547 non-null  float64\n",
      " 17  term_spread_asset_ratio       359547 non-null  float64\n",
      " 18  gdp_qoq_lag1                  359547 non-null  float64\n",
      " 19  cpi_qoq_lag1                  359547 non-null  float64\n",
      " 20  sp500_qoq_lag1                359547 non-null  float64\n",
      " 21  corp_bond_spread_lag1         359547 non-null  float64\n",
      " 22  cons_sentiment_qoq_lag1       359547 non-null  float64\n",
      " 23  unemployment_lag1             359547 non-null  float64\n",
      " 24  unemployment_diff_lag1        359547 non-null  float64\n",
      " 25  household_delinq_lag1         359547 non-null  float64\n",
      " 26  household_delinq_diff_lag1    359547 non-null  float64\n",
      " 27  vix_qoq_lag1                  359547 non-null  float64\n",
      " 28  spread_10y_3m_lag1            359547 non-null  float64\n",
      " 29  log_total_assets_lag1         359547 non-null  float64\n",
      " 30  deposit_ratio_lag1            359547 non-null  float64\n",
      " 31  loan_to_asset_ratio_lag1      359547 non-null  float64\n",
      " 32  equity_to_asset_ratio_lag1    359547 non-null  float64\n",
      " 33  trading_assets_ratio_lag1     359547 non-null  float64\n",
      " 34  roa_lag1                      359547 non-null  float64\n",
      " 35  term_spread_asset_ratio_lag1  359547 non-null  float64\n",
      "dtypes: float64(36)\n",
      "memory usage: 100.1+ MB\n",
      "\n",
      "Sample of the processed DataFrame:\n",
      "                gdp_qoq   cpi_qoq  sp500_qoq  corp_bond_spread  \\\n",
      "id      date                                                     \n",
      "1000052 157.0 -0.003442  0.005316   0.152218            0.0465   \n",
      "        158.0  0.004704  0.008604   0.149850            0.0315   \n",
      "        159.0  0.014006  0.007829   0.054887            0.0286   \n",
      "        160.0  0.007737  0.001585   0.048722            0.0257   \n",
      "        161.0  0.014601 -0.000354  -0.118622            0.0269   \n",
      "\n",
      "               cons_sentiment_qoq  unemployment  unemployment_diff  \\\n",
      "id      date                                                         \n",
      "1000052 157.0            0.169811         0.093              0.010   \n",
      "        158.0            0.002933         0.096              0.003   \n",
      "        159.0            0.026316         0.099              0.003   \n",
      "        160.0            0.052707         0.098             -0.001   \n",
      "        161.0            0.000000         0.096             -0.002   \n",
      "\n",
      "               household_delinq  household_delinq_diff   vix_qoq  ...  \\\n",
      "id      date                                                      ...   \n",
      "1000052 157.0            0.0677                 0.0026 -0.266222  ...   \n",
      "        158.0            0.0649                -0.0028 -0.228044  ...   \n",
      "        159.0            0.0633                -0.0016 -0.094939  ...   \n",
      "        160.0            0.0578                -0.0055 -0.126571  ...   \n",
      "        161.0            0.0510                -0.0068  0.309677  ...   \n",
      "\n",
      "               household_delinq_diff_lag1  vix_qoq_lag1  spread_10y_3m_lag1  \\\n",
      "id      date                                                                  \n",
      "1000052 157.0                      0.0087     -0.232082              0.0253   \n",
      "        158.0                      0.0026     -0.266222              0.0314   \n",
      "        159.0                     -0.0028     -0.228044              0.0336   \n",
      "        160.0                     -0.0016     -0.094939              0.0340   \n",
      "        161.0                     -0.0055     -0.126571              0.0361   \n",
      "\n",
      "               log_total_assets_lag1  deposit_ratio_lag1  \\\n",
      "id      date                                               \n",
      "1000052 157.0              12.548456            0.794834   \n",
      "        158.0              12.536006            0.800981   \n",
      "        159.0              12.539866            0.779850   \n",
      "        160.0              12.527076            0.779947   \n",
      "        161.0              12.537629            0.796911   \n",
      "\n",
      "               loan_to_asset_ratio_lag1  equity_to_asset_ratio_lag1  \\\n",
      "id      date                                                          \n",
      "1000052 157.0                  0.820713                    0.087432   \n",
      "        158.0                  0.817715                    0.086708   \n",
      "        159.0                  0.793902                    0.086564   \n",
      "        160.0                  0.773843                    0.084330   \n",
      "        161.0                  0.687991                    0.080240   \n",
      "\n",
      "               trading_assets_ratio_lag1  roa_lag1  \\\n",
      "id      date                                         \n",
      "1000052 157.0                        0.0  0.002933   \n",
      "        158.0                        0.0  0.002261   \n",
      "        159.0                        0.0  0.002915   \n",
      "        160.0                        0.0  0.002078   \n",
      "        161.0                        0.0  0.002053   \n",
      "\n",
      "               term_spread_asset_ratio_lag1  \n",
      "id      date                                 \n",
      "1000052 157.0                      0.020764  \n",
      "        158.0                      0.025676  \n",
      "        159.0                      0.026675  \n",
      "        160.0                      0.026311  \n",
      "        161.0                      0.024836  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "data_processed = data_winsorized.copy()\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "\n",
    "\n",
    "# Handle potential division by zero or NaNs if total_assets can be zero\n",
    "data_processed['loan_to_asset_ratio'] = data_processed['loan_to_asset_ratio'].replace([np.inf, -np.inf], np.nan) # Or handle upstream\n",
    "data_processed['term_spread_asset_ratio'] = data_processed['spread_10y_3m'] * data_processed['loan_to_asset_ratio']\n",
    "\n",
    "# --- Create lagged variables efficiently ---\n",
    "\n",
    "# Store original columns to iterate over (avoids lagging the lags)\n",
    "original_cols = list(data_processed.columns)\n",
    "lagged_features = [] # List to store the new lagged Series\n",
    "\n",
    "print(\"Starting lag generation...\")\n",
    "# Loop through original columns\n",
    "for col in original_cols:\n",
    "     # Loop through desired lags\n",
    "     for lag in range(1, lags + 1):\n",
    "          # Calculate the lagged series\n",
    "          # Use level='id' to group by the 'id' level of the MultiIndex\n",
    "          lagged_series = data_processed.groupby(level='id')[col].shift(lag)\n",
    "          # Rename the series uniquely\n",
    "          lagged_series.name = f'{col}_lag{lag}'\n",
    "          # Append the series to the list\n",
    "          lagged_features.append(lagged_series)\n",
    "\n",
    "print(f\"Generated {len(lagged_features)} lagged features.\")\n",
    "\n",
    "# Concatenate all new lagged features at once\n",
    "if lagged_features: # Check if the list is not empty\n",
    "    print(\"Concatenating lagged features...\")\n",
    "    # This single concatenation is much more efficient\n",
    "    data_processed = pd.concat([data_processed] + lagged_features, axis=1)\n",
    "    print(\"Concatenation complete.\")\n",
    "else:\n",
    "    print(\"No lagged features were generated.\")\n",
    "\n",
    "# --- Drop NaNs introduced by lagging ---\n",
    "print(f\"Shape before dropping NaNs: {data_processed.shape}\")\n",
    "print(f\"Nr of banks before dropping NaNs: {data_processed.index.get_level_values('id').nunique()}\")\n",
    "data_processed.dropna(inplace=True) # Drop rows with NaNs\n",
    "# Check the shape and number of banks after dropping NaNs\n",
    "print(f\"Shape after dropping NaNs: {data_processed.shape}\")\n",
    "print(f\"Nr of banks after dropping NaNs: {data_processed.index.get_level_values('id').nunique()}\")\n",
    "\n",
    "# Min / max date\n",
    "print(f\"Min date: {data_processed.index.get_level_values('date').min()}\")\n",
    "print(f\"Max date: {data_processed.index.get_level_values('date').max()}\")\n",
    "\n",
    "# Optional: Display info to verify\n",
    "print(\"\\nDataFrame Info after processing:\")\n",
    "data_processed.info()\n",
    "\n",
    "print(\"\\nSample of the processed DataFrame:\")\n",
    "print(data_processed.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84cd7d5",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a099732",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_processed.copy(deep=True)\n",
    "\n",
    "# Define features: all variables except 'roa' with lags 1-4. Use variables list and add _lag1, _lag2, etc.\n",
    "features = []  \n",
    "for lag in range(1, lags + 1):\n",
    "    features += [f\"{var}_lag{lag}\" for var in variables] \n",
    "\n",
    "# Select features\n",
    "X = X[features].copy(deep=True)\n",
    "\n",
    "\n",
    "# Restore default warning behavior if suppressed earlier\n",
    "# warnings.filterwarnings('default')\n",
    "# Select target\n",
    "y = data_processed[['roa']].copy(deep=True) # Target is current ROA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d26cac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking and potentially swapping index levels to ('id', 'date')...\n",
      "Current index names: ['id', 'date']\n",
      "Index levels are already ('id', 'date'). Ensuring index is sorted...\n",
      "Index sorted.\n",
      "Index dtypes after swap/sort: [dtype('O'), dtype('float64')]\n",
      "First 5 index values:\n",
      "MultiIndex([('1000052', 157.0),\n",
      "            ('1000052', 158.0),\n",
      "            ('1000052', 159.0),\n",
      "            ('1000052', 160.0),\n",
      "            ('1000052', 161.0)],\n",
      "           names=['id', 'date'])\n",
      "Current index names: ['id', 'date']\n",
      "Index levels are already ('id', 'date'). Ensuring index is sorted...\n",
      "Index sorted.\n",
      "Index dtypes after swap/sort: [dtype('O'), dtype('float64')]\n",
      "First 5 index values:\n",
      "MultiIndex([('1000052', 157.0),\n",
      "            ('1000052', 158.0),\n",
      "            ('1000052', 159.0),\n",
      "            ('1000052', 160.0),\n",
      "            ('1000052', 161.0)],\n",
      "           names=['id', 'date'])\n",
      "Reducing banks to those with the highest total assets in 2010 and existing over the entire period...\n",
      "Reduced to 0 banks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Check and Swap Index Levels ---\n",
    "print(\"Checking and potentially swapping index levels to ('id', 'date')...\")\n",
    "def swap_index_levels(df):\n",
    "    if isinstance(df.index, pd.MultiIndex) and len(df.index.levels) == 2:\n",
    "        print(f\"Current index names: {df.index.names}\")\n",
    "        if df.index.names == ['date', 'id']:\n",
    "            print(\"Swapping levels from ('date', 'id') to ('id', 'date')...\")\n",
    "            df.index = df.index.swaplevel()\n",
    "            print(\"Levels swapped. Sorting index...\")\n",
    "            df.sort_index(inplace=True)\n",
    "            print(\"Index sorted.\")\n",
    "            print(f\"New index names: {df.index.names}\")\n",
    "        elif df.index.names == ['id', 'date']:\n",
    "            print(\"Index levels are already ('id', 'date'). Ensuring index is sorted...\")\n",
    "            df.sort_index(inplace=True)\n",
    "            print(\"Index sorted.\")\n",
    "        else:\n",
    "            print(f\"Warning: Index names are {df.index.names}. Expected ['date', 'id'] or ['id', 'date']. Assuming ('id', 'date') is desired.\")\n",
    "            df.index.names = ['id', 'date'] # Assign names if None\n",
    "            df.sort_index(inplace=True)\n",
    "        print(f\"Index dtypes after swap/sort: {[lvl.dtype for lvl in df.index.levels]}\")\n",
    "        print(f\"First 5 index values:\\n{df.index[:5]}\")\n",
    "    else:\n",
    "        raise TypeError(f\"DataFrame index must be a 2-level MultiIndex. Found index type: {type(df.index)}\")\n",
    "    return df\n",
    "\n",
    "X = swap_index_levels(X)\n",
    "y = swap_index_levels(y)\n",
    "\n",
    "# Optional: Reduce number of banks to those with the highest total assets in 2010 and existing over the entire period\n",
    "if reduce_banks:\n",
    "    print(\"Reducing banks to those with the highest total assets in 2010 and existing over the entire period...\")\n",
    "    # Filter for banks existing in 2010\n",
    "    banks_2010 = data_processed[data_processed.index.get_level_values('date') == '2010-01-01']\n",
    "    # Get the top 10 banks by total assets\n",
    "    top_banks = banks_2010.groupby(level='id')['log_total_assets'].max().nlargest(reduce_banks_to).index\n",
    "    # Filter the original DataFrame for these banks\n",
    "    X = X[X.index.get_level_values('id').isin(top_banks)]\n",
    "    y = y[y.index.get_level_values('id').isin(top_banks)]\n",
    "    print(f\"Reduced to {len(top_banks)} banks.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b23cb",
   "metadata": {},
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5ff1ad0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute 'year'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m dates = X.index.get_level_values(\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m# Ensure dates align with final X\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# --- Time-based Split ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m train_mask = (\u001b[43mdates\u001b[49m\u001b[43m.\u001b[49m\u001b[43myear\u001b[49m <= \u001b[32m2020\u001b[39m)\n\u001b[32m      6\u001b[39m val_mask = (dates.year == \u001b[32m2022\u001b[39m)\n\u001b[32m      7\u001b[39m test_mask = (dates.year == \u001b[32m2024\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Index' object has no attribute 'year'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Get dates AFTER cleaning NaNs and setting index ---\n",
    "dates = X.index.get_level_values('date') # Ensure dates align with final X\n",
    "\n",
    "# --- Time-based Split ---\n",
    "train_mask = (dates.year <= 2020)\n",
    "val_mask = (dates.year == 2022)\n",
    "test_mask = (dates.year == 2024)\n",
    "\n",
    "X_train = X[train_mask]\n",
    "y_train = y[train_mask]\n",
    "X_val = X[val_mask]\n",
    "y_val = y[val_mask]\n",
    "X_test = X[test_mask]\n",
    "y_test = y[test_mask]\n",
    "\n",
    "print(f\"\\nData Shapes:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val:   {X_val.shape}, y_val:   {y_val.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}, y_test:  {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da876ed",
   "metadata": {},
   "source": [
    "# Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554c3fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the final data\n",
    "X_train.to_parquet(\"X_train.parquet\")\n",
    "y_train.to_parquet(\"y_train.parquet\")\n",
    "X_val.to_parquet(\"X_val.parquet\")\n",
    "y_val.to_parquet(\"y_val.parquet\")\n",
    "X_test.to_parquet(\"X_test.parquet\")\n",
    "y_test.to_parquet(\"y_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2173ef62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sanity checks on splits:\n",
      "X_train dates: 2009Q2 to 2020Q4\n",
      "X_val dates:   2022Q1 to 2022Q4\n",
      "X_test dates:  2024Q1 to 2024Q4\n",
      "y_train dates: 2009Q2 to 2020Q4\n",
      "y_val dates:   2022Q1 to 2022Q4\n",
      "y_test dates:  2024Q1 to 2024Q4\n",
      "\n",
      "Checking for NaNs in final datasets:\n",
      "X_train NaNs: 0\n",
      "y_train NaNs: 0\n",
      "X_val NaNs:   0\n",
      "y_val NaNs:   0\n",
      "X_test NaNs:  0\n",
      "y_test NaNs:  0\n"
     ]
    }
   ],
   "source": [
    "# Run some sanity checks on splits\n",
    "print(f\"\\nSanity checks on splits:\")\n",
    "print(f\"X_train dates: {X_train.index.get_level_values('date').min()} to {X_train.index.get_level_values('date').max()}\")\n",
    "print(f\"X_val dates:   {X_val.index.get_level_values('date').min()} to {X_val.index.get_level_values('date').max()}\")\n",
    "print(f\"X_test dates:  {X_test.index.get_level_values('date').min()} to {X_test.index.get_level_values('date').max()}\")\n",
    "print(f\"y_train dates: {y_train.index.get_level_values('date').min()} to {y_train.index.get_level_values('date').max()}\")\n",
    "print(f\"y_val dates:   {y_val.index.get_level_values('date').min()} to {y_val.index.get_level_values('date').max()}\")\n",
    "print(f\"y_test dates:  {y_test.index.get_level_values('date').min()} to {y_test.index.get_level_values('date').max()}\")\n",
    "# Check for any NaNs in the final datasets\n",
    "print(f\"\\nChecking for NaNs in final datasets:\")\n",
    "print(f\"X_train NaNs: {X_train.isna().sum().sum()}\")\n",
    "print(f\"y_train NaNs: {y_train.isna().sum().sum()}\")\n",
    "print(f\"X_val NaNs:   {X_val.isna().sum().sum()}\")\n",
    "print(f\"y_val NaNs:   {y_val.isna().sum().sum()}\")\n",
    "print(f\"X_test NaNs:  {X_test.isna().sum().sum()}\")\n",
    "print(f\"y_test NaNs:  {y_test.isna().sum().sum()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f096a9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Descriptive Statistics ---\n",
      "\n",
      "--- Mean ---\n",
      "                             Train  Validation    Test\n",
      "cons_sentiment_qoq_lag1     0.0072     -0.0692  0.0012\n",
      "corp_bond_spread_lag1       0.0276      0.0203  0.0162\n",
      "cpi_qoq_lag1                0.0042      0.0201  0.0066\n",
      "deposit_ratio_lag1          0.6986      0.7785  0.7467\n",
      "equity_to_asset_ratio_lag1  0.1334      0.1256  0.1322\n",
      "gdp_qoq_lag1                0.0087      0.0238  0.0123\n",
      "household_delinq_diff_lag1 -0.0008      0.0013  0.0007\n",
      "household_delinq_lag1       0.0323      0.0178  0.0318\n",
      "loan_to_asset_ratio_lag1    0.5946      0.5309  0.5614\n",
      "log_total_assets_lag1      17.5515     18.1985 18.2341\n",
      "roa_lag1                    0.0024      0.0035  0.0030\n",
      "sp500_qoq_lag1              0.0333     -0.0395  0.0771\n",
      "spread_10y_3m_lag1          0.0196      0.0136 -0.0094\n",
      "trading_assets_ratio_lag1   0.0120      0.0104  0.0107\n",
      "unemployment_diff_lag1      0.0004     -0.0040  0.0012\n",
      "unemployment_lag1           0.0677      0.0378  0.0395\n",
      "vix_qoq_lag1                0.0067      0.0892  0.0390\n",
      "\n",
      "--- Standard Deviation ---\n",
      "                            Train  Validation   Test\n",
      "cons_sentiment_qoq_lag1    0.0655      0.0246 0.1205\n",
      "corp_bond_spread_lag1      0.0069      0.0017 0.0010\n",
      "cpi_qoq_lag1               0.0044      0.0041 0.0020\n",
      "deposit_ratio_lag1         0.1779      0.1713 0.1674\n",
      "equity_to_asset_ratio_lag1 0.0885      0.1366 0.1404\n",
      "gdp_qoq_lag1               0.0178      0.0073 0.0008\n",
      "household_delinq_diff_lag1 0.0025      0.0006 0.0007\n",
      "household_delinq_lag1      0.0140      0.0018 0.0005\n",
      "loan_to_asset_ratio_lag1   0.1914      0.2007 0.1970\n",
      "log_total_assets_lag1      1.3488      1.4850 1.5610\n",
      "roa_lag1                   0.0062      0.0045 0.0050\n",
      "sp500_qoq_lag1             0.0808      0.0970 0.0307\n",
      "spread_10y_3m_lag1         0.0099      0.0054 0.0012\n",
      "trading_assets_ratio_lag1  0.0428      0.0258 0.0279\n",
      "unemployment_diff_lag1     0.0143      0.0031 0.0008\n",
      "unemployment_lag1          0.0236      0.0027 0.0017\n",
      "vix_qoq_lag1               0.2963      0.1471 0.1166\n",
      "\n",
      "--- Median ---\n",
      "                             Train  Validation    Test\n",
      "cons_sentiment_qoq_lag1     0.0033     -0.0740 -0.0575\n",
      "corp_bond_spread_lag1       0.0267      0.0201  0.0162\n",
      "cpi_qoq_lag1                0.0045      0.0217  0.0069\n",
      "deposit_ratio_lag1          0.7517      0.8306  0.7959\n",
      "equity_to_asset_ratio_lag1  0.1181      0.0966  0.1001\n",
      "gdp_qoq_lag1                0.0100      0.0234  0.0121\n",
      "household_delinq_diff_lag1 -0.0006      0.0012  0.0007\n",
      "household_delinq_lag1       0.0254      0.0169  0.0319\n",
      "loan_to_asset_ratio_lag1    0.6492      0.5600  0.6142\n",
      "log_total_assets_lag1      17.2227     18.2860 18.1906\n",
      "roa_lag1                    0.0024      0.0026  0.0025\n",
      "sp500_qoq_lag1              0.0469     -0.0495  0.0784\n",
      "spread_10y_3m_lag1          0.0204      0.0156 -0.0094\n",
      "trading_assets_ratio_lag1   0.0011      0.0006  0.0006\n",
      "unemployment_diff_lag1     -0.0010     -0.0040  0.0015\n",
      "unemployment_lag1           0.0670      0.0380  0.0390\n",
      "vix_qoq_lag1               -0.0607      0.0671  0.0192\n",
      "\n",
      "--- Minimum ---\n",
      "                             Train  Validation    Test\n",
      "cons_sentiment_qoq_lag1    -0.2329     -0.0973 -0.0880\n",
      "corp_bond_spread_lag1       0.0171      0.0178  0.0148\n",
      "cpi_qoq_lag1               -0.0098      0.0132  0.0035\n",
      "deposit_ratio_lag1          0.0000      0.0000  0.0000\n",
      "equity_to_asset_ratio_lag1  0.0376      0.0297  0.0514\n",
      "gdp_qoq_lag1               -0.0825      0.0177  0.0116\n",
      "household_delinq_diff_lag1 -0.0068      0.0004 -0.0004\n",
      "household_delinq_lag1       0.0199      0.0157  0.0310\n",
      "loan_to_asset_ratio_lag1    0.0000      0.0000  0.0000\n",
      "log_total_assets_lag1      13.8950     14.0685 14.1079\n",
      "roa_lag1                   -0.1090     -0.0024 -0.0235\n",
      "sp500_qoq_lag1             -0.2000     -0.1645  0.0392\n",
      "spread_10y_3m_lag1         -0.0018      0.0045 -0.0107\n",
      "trading_assets_ratio_lag1   0.0000      0.0000  0.0000\n",
      "unemployment_diff_lag1     -0.0420     -0.0090  0.0000\n",
      "unemployment_lag1           0.0360      0.0350  0.0380\n",
      "vix_qoq_lag1               -0.3921     -0.0953 -0.1033\n",
      "\n",
      "--- Maximum ---\n",
      "                             Train  Validation    Test\n",
      "cons_sentiment_qoq_lag1     0.1698     -0.0311  0.2080\n",
      "corp_bond_spread_lag1       0.0549      0.0225  0.0175\n",
      "cpi_qoq_lag1                0.0114      0.0239  0.0091\n",
      "deposit_ratio_lag1          0.9162      0.9507  0.9291\n",
      "equity_to_asset_ratio_lag1  0.9417      0.9610  0.9610\n",
      "gdp_qoq_lag1                0.0877      0.0357  0.0137\n",
      "household_delinq_diff_lag1  0.0087      0.0022  0.0016\n",
      "household_delinq_lag1       0.0677      0.0205  0.0324\n",
      "loan_to_asset_ratio_lag1    0.9946      0.8847  0.8590\n",
      "log_total_assets_lag1      21.7777     21.9694 21.9998\n",
      "roa_lag1                    0.0907      0.0376  0.0390\n",
      "sp500_qoq_lag1              0.1995      0.1065  0.1124\n",
      "spread_10y_3m_lag1          0.0361      0.0185 -0.0080\n",
      "trading_assets_ratio_lag1   0.5995      0.1998  0.1377\n",
      "unemployment_diff_lag1      0.0920     -0.0010  0.0020\n",
      "unemployment_lag1           0.1300      0.0420  0.0420\n",
      "vix_qoq_lag1                1.2316      0.3159  0.2210\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Assuming X_train, X_val, X_test are already defined ---\n",
    "# If not, uncomment and adapt the example creation/loading code:\n",
    "# Example DataFrames (replace with your actual data)\n",
    "# Make sure they have MultiIndex including 'date' or similar structure\n",
    "# dates_train = pd.to_datetime(pd.date_range('2021-01-01', '2022-12-31', freq='QE'))\n",
    "# dates_val = pd.to_datetime(pd.date_range('2023-01-01', '2023-12-31', freq='QE'))\n",
    "# dates_test = pd.to_datetime(pd.date_range('2024-01-01', '2024-12-31', freq='QE'))\n",
    "# ids = ['bank_a', 'bank_b']\n",
    "\n",
    "# index_train = pd.MultiIndex.from_product([ids, dates_train], names=['id', 'date'])\n",
    "# index_val = pd.MultiIndex.from_product([ids, dates_val], names=['id', 'date'])\n",
    "# index_test = pd.MultiIndex.from_product([ids, dates_test], names=['id', 'date'])\n",
    "\n",
    "# X_train = pd.DataFrame(np.random.rand(len(index_train), 3), index=index_train, columns=['feature_c', 'feature_a', 'feature_b'])\n",
    "# X_val = pd.DataFrame(np.random.rand(len(index_val), 3), index=index_val, columns=['feature_c', 'feature_a', 'feature_b'])\n",
    "# X_test = pd.DataFrame(np.random.rand(len(index_test), 3), index=index_test, columns=['feature_c', 'feature_a', 'feature_b'])\n",
    "# import numpy as np # Don't forget this if using the example above\n",
    "\n",
    "# --- Calculate Descriptive Statistics ---\n",
    "\n",
    "datasets = {'Train': X_train, 'Validation': X_val, 'Test': X_test}\n",
    "stats_to_compute = {\n",
    "    'Mean': lambda df: df.mean(numeric_only=True),\n",
    "    'Standard Deviation': lambda df: df.std(numeric_only=True),\n",
    "    'Median': lambda df: df.median(numeric_only=True),\n",
    "    'Minimum': lambda df: df.min(numeric_only=True),\n",
    "    'Maximum': lambda df: df.max(numeric_only=True)\n",
    "}\n",
    "\n",
    "# Set display precision for floats\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}')\n",
    "\n",
    "# Get sorted feature names (assuming all datasets have the same features)\n",
    "# Use X_train as reference, filter out non-numeric if necessary later\n",
    "all_features = X_train.columns.tolist()\n",
    "# Attempt to identify numeric features to avoid errors with .mean() etc.\n",
    "# This is safer if non-numeric columns might exist\n",
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "sorted_features = sorted(numeric_features)\n",
    "\n",
    "\n",
    "print(\"\\n--- Descriptive Statistics ---\")\n",
    "\n",
    "for stat_name, stat_func in stats_to_compute.items():\n",
    "    print(f\"\\n--- {stat_name} ---\")\n",
    "    summary_df = pd.DataFrame(index=sorted_features) # Ensure index is sorted features\n",
    "\n",
    "    for ds_name, ds_df in datasets.items():\n",
    "        if not ds_df.empty:\n",
    "            # Calculate stat only for sorted numeric features\n",
    "            # Use .reindex() to ensure consistent feature order and handle missing cols\n",
    "            stats_series = stat_func(ds_df[sorted_features])\n",
    "            summary_df[ds_name] = stats_series.reindex(sorted_features)\n",
    "        else:\n",
    "            # Add a column of NaNs if the dataset is empty\n",
    "             summary_df[ds_name] = pd.NA\n",
    "\n",
    "    # Display the table for the current statistic\n",
    "    # Drop rows where all values are NaN (might happen if a feature was non-numeric)\n",
    "    print(summary_df.dropna(how='all'))\n",
    "\n",
    "\n",
    "# Reset display options if needed elsewhere\n",
    "# pd.reset_option('display.float_format')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
