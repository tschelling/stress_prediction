{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fbd95c2",
   "metadata": {},
   "source": [
    "# Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07b9c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script to train, evaluate, and compare multiple regression models\n",
    "(Pooled OLS, Bank FE, Random Forest, XGBoost) on panel data.\n",
    "\n",
    "Loads pre-split and preprocessed data (X_train, y_train, etc.).\n",
    "Assumes data is clean and aligned. Trains, tunes (optional),\n",
    "predicts, evaluates, and saves models using simplified functions,\n",
    "but ensures constant is handled correctly for OLS.\n",
    "\"\"\"\n",
    "\n",
    "%pip install scikit-learn --quiet\n",
    "\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import xgboost as xgb\n",
    "from linearmodels.panel import PanelOLS\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import (mean_absolute_error, mean_squared_error,\n",
    "                             r2_score)\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "import traceback # Keep for debugging potential errors\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "\n",
    "# --- File Paths ---\n",
    "DATA_PATHS = {\n",
    "    'X_train': 'X_train.parquet', 'y_train': 'y_train.parquet',\n",
    "    'X_val':   'X_val.parquet',   'y_val':   'y_val.parquet',\n",
    "    'X_test':  'X_test.parquet',  'y_test':  'y_test.parquet',\n",
    "}\n",
    "SAVE_DIR = 'models'\n",
    "# --- Data Structure ---\n",
    "ENTITY_ID_COL = 'id'\n",
    "TIME_COL = 'date'\n",
    "# --- Model Selection ---\n",
    "MODELS_TO_TRAIN = {\n",
    "    'Pooled OLS': True,\n",
    "    'Bank FE': True,\n",
    "    'Random Forest': True,\n",
    "    'XGBoost': True,\n",
    "}\n",
    "# --- Tuning Configuration ---\n",
    "DO_TUNING = True\n",
    "N_ITER_RANDOM_SEARCH = 100\n",
    "CV_FOLDS = 5\n",
    "SCORING_METRIC = 'neg_root_mean_squared_error'\n",
    "# --- Default Model Parameters ---\n",
    "RF_DEFAULT_PARAMS = { 'n_estimators': 200, 'max_depth': 30, 'min_samples_leaf': 10, 'n_jobs': -1, 'random_state': 42 }\n",
    "\n",
    "# Best XGBoost Parameters found: {'colsample_bytree': 0.8822634555704314, 'gamma': 0.08526206184364576, 'learning_rate': 0.023010318597055907, 'max_depth': 6, 'n_estimators': 188, 'reg_alpha': 0.9656320330745594, 'reg_lambda': 0.8083973481164611, 'subsample': 0.7913841307520112}\n",
    "  \n",
    "XGB_DEFAULT_PARAMS = { 'n_estimators': 200, 'learning_rate': 0.1,   'max_depth': 7, 'subsample': 0.8, 'colsample_bytree': 0.8, 'objective': 'reg:squarederror', 'n_jobs': -1, 'random_state': 42, 'early_stopping_rounds': 10, 'tree_method': 'hist' }\n",
    "XGB_DEFAULT_PARAMS = { 'n_estimators': 188, 'learning_rate': 0.023, 'max_depth': 6, 'subsample': 0.9, 'colsample_bytree': 0.8, 'objective': 'reg:squarederror', 'n_jobs': -1, 'random_state': 42, 'early_stopping_rounds': 10, 'tree_method': 'hist' }\n",
    "\n",
    "# --- Random Search Parameter Distributions ---\n",
    "RF_PARAM_DIST = { 'n_estimators': randint(100, 500), 'max_depth': randint(10, 50), 'min_samples_split': randint(2, 20), 'min_samples_leaf': randint(5, 30), 'max_features': uniform(0.6, 0.4) }\n",
    "XGB_PARAM_DIST = { 'n_estimators': randint(100, 600), 'learning_rate': uniform(0.01, 0.2), 'max_depth': randint(3, 12), 'subsample': uniform(0.7, 0.3), 'colsample_bytree': uniform(0.7, 0.3), 'gamma': uniform(0, 0.5), 'reg_alpha': uniform(0, 1), 'reg_lambda': uniform(0, 1) }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd10f093",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca47060",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Helper Functions\n",
    "# =============================================================================\n",
    "\n",
    "def check_and_convert_index(df, df_name):\n",
    "    \"\"\"Minimal check: Validates MultiIndex structure and ensures time level is Timestamp.\"\"\"\n",
    "    print(f\"Checking index for {df_name}...\")\n",
    "    if df is None or df.empty: return df\n",
    "    if not isinstance(df.index, pd.MultiIndex): return df\n",
    "    expected_names = [ENTITY_ID_COL, TIME_COL]\n",
    "    if list(df.index.names) != expected_names: warnings.warn(f\"{df_name} index names {df.index.names}, expected {expected_names}.\")\n",
    "    try:\n",
    "        time_level = df.index.get_level_values(TIME_COL)\n",
    "        if pd.api.types.is_period_dtype(time_level.dtype):\n",
    "            print(f\"  Converting PeriodDtype index in {df_name} to Timestamp...\")\n",
    "            entity_level = df.index.get_level_values(ENTITY_ID_COL)\n",
    "            new_time_level = time_level.to_timestamp()\n",
    "            df.index = pd.MultiIndex.from_arrays([entity_level, new_time_level], names=expected_names)\n",
    "            df.sort_index(inplace=True)\n",
    "        elif not pd.api.types.is_datetime64_any_dtype(time_level.dtype):\n",
    "             print(f\"  Warning: Non-standard time index in {df_name}. Attempting conversion.\")\n",
    "             entity_level = df.index.get_level_values(ENTITY_ID_COL)\n",
    "             new_time_level = pd.to_datetime(time_level.to_series(keep_tz=True))\n",
    "             df.index = pd.MultiIndex.from_arrays([entity_level, new_time_level], names=expected_names)\n",
    "             df.sort_index(inplace=True)\n",
    "    except Exception as e: print(f\"ERROR processing time index for {df_name}: {e}\"); raise\n",
    "    return df\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred, model_name, data_set_name):\n",
    "    \"\"\"Calculates key regression metrics. Assumes aligned, non-empty inputs.\"\"\"\n",
    "    print(f\"  Evaluating {model_name} on {data_set_name} set...\")\n",
    "    if y_pred is None: # Keep basic check for valid predictions\n",
    "        print(\"    Skipping evaluation: No predictions provided.\")\n",
    "        return {'R2': np.nan, 'MSE': np.nan, 'RMSE': np.nan, 'MAE': np.nan}\n",
    "    y_true_aligned, y_pred_aligned = y_true.align(y_pred, join='inner', copy=False)\n",
    "    mask = y_true_aligned.notna() & y_pred_aligned.notna()\n",
    "    y_true_eval = y_true_aligned[mask]\n",
    "    y_pred_eval = y_pred_aligned[mask]\n",
    "    if y_true_eval.empty:\n",
    "        print(\"    Skipping evaluation: No valid aligned pairs after dropping NaNs.\")\n",
    "        return {'R2': np.nan, 'MSE': np.nan, 'RMSE': np.nan, 'MAE': np.nan}\n",
    "    mse = mean_squared_error(y_true_eval, y_pred_eval)\n",
    "    mae = mean_absolute_error(y_true_eval, y_pred_eval)\n",
    "    r2 = r2_score(y_true_eval, y_pred_eval)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"    R2: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f} ({len(y_true_eval)} obs)\")\n",
    "    return {'R2': r2, 'MSE': mse, 'RMSE': rmse, 'MAE': mae}\n",
    "\n",
    "# display_results and save_model_artifacts remain the same\n",
    "def display_results(evaluation_results):\n",
    "    \"\"\"Consolidates evaluation results into a DataFrame and prints a summary table.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*40); print(\" Consolidated Evaluation Results \"); print(\"=\"*40)\n",
    "    results_list = []\n",
    "    for model, datasets_metrics in evaluation_results.items():\n",
    "        for dataset, metrics in datasets_metrics.items():\n",
    "            row = metrics.copy(); row['Model'] = model; row['Dataset'] = dataset\n",
    "            results_list.append(row)\n",
    "    if not results_list: print(\"No evaluation results.\"); return None\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    cols_order = ['Model', 'Dataset', 'R2', 'RMSE', 'MAE', 'MSE']\n",
    "    available_cols = [col for col in cols_order if col in results_df.columns]\n",
    "    results_df = results_df[available_cols]\n",
    "    try:\n",
    "        numeric_metrics = ['R2', 'RMSE', 'MAE', 'MSE']\n",
    "        value_cols = [m for m in numeric_metrics if m in results_df.columns]\n",
    "        if value_cols and 'Model' in results_df.columns and 'Dataset' in results_df.columns:\n",
    "            results_agg = results_df.groupby(['Model', 'Dataset'], observed=False, as_index=False)[value_cols].mean()\n",
    "            results_pivot = results_agg.pivot(index='Model', columns='Dataset', values=value_cols)\n",
    "            results_pivot.columns = ['_'.join(map(str, col)).strip() for col in results_pivot.columns.values]\n",
    "            try: # Attempt sorting\n",
    "                dataset_order = pd.CategoricalDtype(['train', 'val', 'test'], ordered=True)\n",
    "                metric_order = pd.CategoricalDtype(['R2', 'RMSE', 'MAE', 'MSE'], ordered=True)\n",
    "                col_metrics = results_pivot.columns.str.split('_').str[0]\n",
    "                col_datasets = results_pivot.columns.str.split('_').str[-1]\n",
    "                col_metrics_cat = pd.Categorical(col_metrics, categories=metric_order.categories, ordered=True)\n",
    "                col_datasets_cat = pd.Categorical(col_datasets, categories=dataset_order.categories, ordered=True)\n",
    "                sort_indices = np.lexsort((col_datasets_cat.codes, col_metrics_cat.codes))\n",
    "                results_pivot = results_pivot.iloc[:, sort_indices]\n",
    "            except Exception as sort_e: print(f\"  Warning: Could not sort pivot table cols ({sort_e}).\")\n",
    "            print(\"\\n--- Performance Summary (Metrics by Dataset) ---\")\n",
    "            with pd.option_context('display.float_format', '{:.4f}'.format): print(results_pivot)\n",
    "            return results_pivot\n",
    "        else: # Fallback display\n",
    "            print(\"\\nNo numeric metrics or pivot columns. Raw results:\")\n",
    "            with pd.option_context('display.float_format', '{:.4f}'.format): print(results_df)\n",
    "            return results_df\n",
    "    except Exception as e: # Fallback display on pivot error\n",
    "        print(\"\\nCould not create pivot table. Raw results:\")\n",
    "        with pd.option_context('display.float_format', '{:.4f}'.format): print(results_df); print(f\"(Pivot error: {e})\")\n",
    "        return results_df\n",
    "\n",
    "def save_model_artifacts(model, model_name, best_params, save_dir, timestamp_prefix):\n",
    "    \"\"\"Saves model and parameters (simplified error handling).\"\"\"\n",
    "    if model is None: return\n",
    "    print(f\"\\n--- Saving Artifacts for {model_name} ---\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model_filename_base = model_name.replace(' ', '_').lower()\n",
    "    model_filename = f\"{timestamp_prefix}_{model_filename_base}_model.joblib\"\n",
    "    save_path = os.path.join(save_dir, model_filename)\n",
    "    joblib.dump(model, save_path)\n",
    "    print(f\"  Saved model to '{save_path}'\")\n",
    "    if best_params:\n",
    "         param_filename = f\"{timestamp_prefix}_{model_filename_base}_best_params.json\"\n",
    "         param_save_path = os.path.join(save_dir, param_filename)\n",
    "         try:\n",
    "             serializable_params = {k: (int(v) if isinstance(v, np.integer) else float(v) if isinstance(v, np.floating) else v) for k, v in best_params.items()}\n",
    "             with open(param_save_path, 'w') as f: json.dump(serializable_params, f, indent=4)\n",
    "             print(f\"  Saved best parameters to '{param_save_path}'\")\n",
    "         except Exception as e_p: print(f\"  ERROR saving best parameters for {model_name}: {e_p}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Model Training Functions\n",
    "# =============================================================================\n",
    "\n",
    "def train_pooled_ols(X_train, y_train):\n",
    "    \"\"\"Trains Pooled OLS. Assumes X_train has features, y_train is target Series. Adds constant.\"\"\"\n",
    "    print(\"\\n--- Training Pooled OLS ---\")\n",
    "    y_train_ols = y_train.squeeze()\n",
    "    # Add constant before fitting\n",
    "    X_train_ols = sm.add_constant(X_train, has_constant='add')\n",
    "    ols_model = sm.OLS(y_train_ols, X_train_ols)\n",
    "    ols_results = ols_model.fit()\n",
    "    print(f\"Pooled OLS model fitted using {ols_results.nobs} observations.\")\n",
    "    # Return model and the names of parameters actually fitted (including 'const')\n",
    "    return ols_results, ols_results.params.index.tolist()\n",
    "\n",
    "def train_bank_fe(X_train, y_train):\n",
    "    \"\"\"Trains Bank Fixed Effects. Assumes clean, aligned X_train, y_train with MultiIndex.\"\"\"\n",
    "    print(\"\\n--- Training Bank Fixed Effects (FE) ---\")\n",
    "    y_train_fe = y_train.squeeze()\n",
    "    fe_model = PanelOLS(y_train_fe, X_train, entity_effects=True, drop_absorbed=True)\n",
    "    fe_results = fe_model.fit(cov_type='clustered', cluster_entity=True)\n",
    "    print(f\"Bank Fixed Effects model fitted using {fe_results.nobs} observations.\")\n",
    "    effective_features = fe_results.params.index.tolist() # Get features used post-absorption\n",
    "    absorbed_vars = list(set(X_train.columns) - set(effective_features))\n",
    "    if absorbed_vars: print(f\"  Note: Variables likely absorbed by FE: {absorbed_vars}\")\n",
    "    # Return model and names of parameters fitted (effective features)\n",
    "    return fe_results, effective_features\n",
    "\n",
    "# RF and XGBoost training functions \n",
    "def train_random_forest(X_train, y_train, entity_col, tune_config):\n",
    "    \"\"\"Tunes (optional) and trains Random Forest. Assumes clean, aligned X/y.\"\"\"\n",
    "    print(\"\\n--- Training Pooled Random Forest ---\")\n",
    "    y_train_rf = y_train.squeeze().values.ravel()\n",
    "    groups_rf = X_train.index.get_level_values(entity_col)\n",
    "    X_train_rf = X_train\n",
    "\n",
    "    best_params = None\n",
    "    current_rf_params = RF_DEFAULT_PARAMS.copy()\n",
    "    rf_model = RandomForestRegressor(**current_rf_params, verbose=0)\n",
    "\n",
    "    if tune_config['do_tuning']:\n",
    "        print(f\"  Running Randomized Search CV ({tune_config['n_iter']} iterations)...\")\n",
    "        group_kfold = GroupKFold(n_splits=tune_config['cv_folds'])\n",
    "        base_estimator = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "        rf_random_search = RandomizedSearchCV(\n",
    "            estimator=base_estimator, param_distributions=RF_PARAM_DIST,\n",
    "            n_iter=tune_config['n_iter'], cv=group_kfold, scoring=tune_config['scoring_metric'],\n",
    "            n_jobs=-1, random_state=42, verbose=1, error_score='raise'\n",
    "        )\n",
    "        rf_random_search.fit(X_train_rf, y_train_rf, groups=groups_rf)\n",
    "        print(f\"  Best RF Parameters found: {rf_random_search.best_params_}\")\n",
    "        print(f\"  Best RF CV Score ({tune_config['scoring_metric']}): {rf_random_search.best_score_:.4f}\")\n",
    "        rf_model = rf_random_search.best_estimator_\n",
    "        best_params = rf_random_search.best_params_\n",
    "    else:\n",
    "        print(\"  Skipping tuning, using default parameters.\")\n",
    "        rf_model.fit(X_train_rf, y_train_rf)\n",
    "        print(f\"Random Forest fitted with default parameters.\")\n",
    "\n",
    "    if hasattr(rf_model, 'feature_importances_'):\n",
    "        importances = pd.Series(rf_model.feature_importances_, index=X_train_rf.columns)\n",
    "        print(\"\\n  --- Random Forest Feature Importances (Top 15) ---\")\n",
    "        print(importances.nlargest(15).to_string())\n",
    "\n",
    "    # Return model, params, and columns used (all columns from X_train)\n",
    "    return rf_model, best_params, X_train_rf.columns.tolist()\n",
    "\n",
    "\n",
    "def train_xgboost(X_train, y_train, entity_col, tune_config, X_val=None, y_val=None):\n",
    "    \"\"\"Tunes (optional) and trains XGBoost. Handles NaN targets minimally.\"\"\"\n",
    "    print(\"\\n--- Training Pooled XGBoost ---\")\n",
    "    target_col_name = y_train.squeeze().name\n",
    "\n",
    "    # Minimal cleaning: Join X/y, drop rows with NaN target, split back\n",
    "    df_train_temp = X_train.join(y_train.squeeze().rename(target_col_name), how='inner')\n",
    "    df_train_temp.dropna(subset=[target_col_name], inplace=True)\n",
    "    X_train_xgb_clean = df_train_temp[X_train.columns]\n",
    "    y_train_xgb_clean = df_train_temp[target_col_name]\n",
    "    groups_xgb = df_train_temp.index.get_level_values(entity_col)\n",
    "    del df_train_temp\n",
    "\n",
    "    if y_train_xgb_clean.empty: print(\"ERROR: No training data after dropping NaN targets.\"); return None, None, None\n",
    "\n",
    "    # Prepare validation set\n",
    "    eval_set = None\n",
    "    fit_params = {}\n",
    "    if X_val is not None and y_val is not None:\n",
    "        print(\"  Preparing validation set...\")\n",
    "        df_val_temp = X_val.join(y_val.squeeze().rename(target_col_name), how='inner')\n",
    "        df_val_temp.dropna(subset=[target_col_name], inplace=True)\n",
    "        if not df_val_temp.empty:\n",
    "            X_val_xgb_clean = df_val_temp[X_train.columns] # Ensure same columns as train\n",
    "            y_val_xgb_clean = df_val_temp[target_col_name]\n",
    "            eval_set = [(X_train_xgb_clean, y_train_xgb_clean), (X_val_xgb_clean, y_val_xgb_clean)]\n",
    "            print(\"  Validation set prepared.\")\n",
    "        else: print(\"  Validation set empty after dropping NaN targets.\")\n",
    "        del df_val_temp\n",
    "    else: print(\"  No validation data provided.\")\n",
    "\n",
    "    best_params = None\n",
    "    current_xgb_params = XGB_DEFAULT_PARAMS.copy()\n",
    "    xgb_model = None\n",
    "\n",
    "    if tune_config['do_tuning']:\n",
    "        print(f\"  Running Randomized Search CV ({tune_config['n_iter']} iterations)...\")\n",
    "        group_kfold = GroupKFold(n_splits=tune_config['cv_folds'])\n",
    "        xgb_base = xgb.XGBRegressor(objective='reg:squarederror', n_jobs=-1, random_state=42, tree_method=current_xgb_params.get('tree_method', 'auto'))\n",
    "        xgb_random_search = RandomizedSearchCV(\n",
    "            estimator=xgb_base, param_distributions=XGB_PARAM_DIST,\n",
    "            n_iter=tune_config['n_iter'], cv=group_kfold, scoring=tune_config['scoring_metric'],\n",
    "            n_jobs=-1, random_state=42, verbose=1, error_score='raise'\n",
    "        )\n",
    "        xgb_random_search.fit(X_train_xgb_clean, y_train_xgb_clean, groups=groups_xgb)\n",
    "        print(f\"  Best XGBoost Parameters found: {xgb_random_search.best_params_}\")\n",
    "        print(f\"  Best XGBoost CV Score ({tune_config['scoring_metric']}): {xgb_random_search.best_score_:.4f}\")\n",
    "        best_params = xgb_random_search.best_params_\n",
    "\n",
    "        print(\"  Refitting best model on full training data...\")\n",
    "        final_params = best_params.copy()\n",
    "        final_params.update({k: v for k, v in current_xgb_params.items() if k in ['tree_method', 'objective', 'random_state', 'n_jobs']})\n",
    "        fit_final_params = {'verbose': False}\n",
    "        if eval_set:\n",
    "            final_params['early_stopping_rounds'] = current_xgb_params.get('early_stopping_rounds', 10)\n",
    "            fit_final_params['eval_set'] = eval_set\n",
    "        xgb_model = xgb.XGBRegressor(**final_params)\n",
    "        xgb_model.fit(X_train_xgb_clean, y_train_xgb_clean, **fit_final_params)\n",
    "        if eval_set and hasattr(xgb_model, 'best_iteration'):\n",
    "             print(f\"    Refit completed. Best iteration: {xgb_model.best_iteration}\")\n",
    "             best_params['n_estimators'] = xgb_model.best_iteration\n",
    "        else: print(\"    Refit completed.\")\n",
    "    else:\n",
    "        print(\"  Skipping tuning, using default parameters.\")\n",
    "        xgb_model = xgb.XGBRegressor(**current_xgb_params)\n",
    "        fit_params_default = {'verbose': False}\n",
    "        if eval_set: fit_params_default['eval_set'] = eval_set\n",
    "        xgb_model.fit(X_train_xgb_clean, y_train_xgb_clean, **fit_params_default)\n",
    "        print(f\"XGBoost fitted with default parameters.\")\n",
    "        if eval_set and hasattr(xgb_model, 'best_iteration'): print(f\"  Best iteration: {xgb_model.best_iteration}\")\n",
    "\n",
    "    return xgb_model, best_params, X_train.columns.tolist()\n",
    "\n",
    "\n",
    "# run_predictions modified to accept train_cols again for OLS/FE\n",
    "def run_predictions(model, model_name, X_data, train_cols=None):\n",
    "    \"\"\"\n",
    "    Generates predictions. Assumes X_data is clean.\n",
    "    Requires train_cols for OLS/FE to specify columns (incl const/effective features).\n",
    "    For RF/XGB, assumes X_data contains the correct features.\n",
    "    \"\"\"\n",
    "    print(f\"  Generating predictions for {model_name}...\")\n",
    "    if model is None or X_data is None or X_data.empty: return None\n",
    "\n",
    "    predictions = None\n",
    "    try:\n",
    "        if model_name == 'Pooled OLS':\n",
    "            if train_cols is None: raise ValueError(\"train_cols (incl const) needed for OLS.\")\n",
    "            # Ensure input X_data has columns from train_cols (excluding const) before adding constant\n",
    "            feature_cols_ols = [c for c in train_cols if c != 'const']\n",
    "            X_pred_ols = sm.add_constant(X_data[feature_cols_ols], has_constant='add')\n",
    "            # Align to the exact columns from training (incl 'const' and correct order)\n",
    "            X_pred_ols = X_pred_ols.reindex(columns=train_cols, fill_value=np.nan)\n",
    "            predictions = model.predict(X_pred_ols)\n",
    "\n",
    "        elif model_name == 'Bank FE':\n",
    "            if train_cols is None: raise ValueError(\"train_cols (effective features) needed for FE.\")\n",
    "            # Select only the effective feature columns used in training\n",
    "            X_pred_fe = X_data[train_cols]\n",
    "            # Predict using the effective features\n",
    "            predictions = model.predict(exog=X_pred_fe)\n",
    "            if isinstance(predictions, pd.DataFrame): predictions = predictions.iloc[:, 0]\n",
    "            # Reindex to original X_data index (predict might drop rows if NaNs were present)\n",
    "            # Assuming model.predict doesn't drop rows if input is clean\n",
    "            predictions = pd.Series(predictions, index=X_pred_fe.index).reindex(X_data.index)\n",
    "\n",
    "        elif model_name in ['Random Forest', 'XGBoost']:\n",
    "            # Assumes X_data contains exactly the columns the model was trained on\n",
    "            # If train_cols were provided (e.g., X_train.columns), ensure X_data matches\n",
    "            if train_cols and set(X_data.columns) != set(train_cols):\n",
    "                 warnings.warn(f\"Columns in X_data for {model_name} prediction might not match training columns. Ensure order/content is correct.\")\n",
    "                 # Reindex just in case, although ideally X_data is already correct\n",
    "                 X_data = X_data.reindex(columns=train_cols, fill_value=np.nan)\n",
    "\n",
    "            predictions = model.predict(X_data)\n",
    "\n",
    "        # Return as a Series with original index\n",
    "        if predictions is not None:\n",
    "            predictions = pd.Series(predictions, index=X_data.index, name=f\"{model_name}_pred\")\n",
    "            print(f\"  Generated {predictions.notna().sum()} non-NaN {model_name} predictions.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR during prediction generation for {model_name}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92805d05",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208bc01d",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab0b6405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started at: 2025-05-17 18:17:50.322046\n",
      "Saving directory: 'models'\n",
      "\n",
      "--- Loading Pre-Split Datasets & Checking Index ---\n",
      "Checking index for X_train...\n",
      "  Converting PeriodDtype index in X_train to Timestamp...\n",
      "Checking index for y_train...\n",
      "  Converting PeriodDtype index in y_train to Timestamp...\n",
      "Checking index for X_val...\n",
      "  Converting PeriodDtype index in X_val to Timestamp...\n",
      "Checking index for y_val...\n",
      "  Converting PeriodDtype index in y_val to Timestamp...\n",
      "Checking index for X_test...\n",
      "  Converting PeriodDtype index in X_test to Timestamp...\n",
      "Checking index for y_test...\n",
      "  Converting PeriodDtype index in y_test to Timestamp...\n",
      "\n",
      "--- Identifying Target Variable ---\n",
      "Target variable identified as: 'roa'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tansc\\AppData\\Local\\Temp\\ipykernel_12184\\2146223430.py:14: DeprecationWarning: is_period_dtype is deprecated and will be removed in a future version. Use `isinstance(dtype, pd.PeriodDtype)` instead\n",
      "  if pd.api.types.is_period_dtype(time_level.dtype):\n",
      "C:\\Users\\tansc\\AppData\\Local\\Temp\\ipykernel_12184\\2146223430.py:14: DeprecationWarning: is_period_dtype is deprecated and will be removed in a future version. Use `isinstance(dtype, pd.PeriodDtype)` instead\n",
      "  if pd.api.types.is_period_dtype(time_level.dtype):\n",
      "C:\\Users\\tansc\\AppData\\Local\\Temp\\ipykernel_12184\\2146223430.py:14: DeprecationWarning: is_period_dtype is deprecated and will be removed in a future version. Use `isinstance(dtype, pd.PeriodDtype)` instead\n",
      "  if pd.api.types.is_period_dtype(time_level.dtype):\n",
      "C:\\Users\\tansc\\AppData\\Local\\Temp\\ipykernel_12184\\2146223430.py:14: DeprecationWarning: is_period_dtype is deprecated and will be removed in a future version. Use `isinstance(dtype, pd.PeriodDtype)` instead\n",
      "  if pd.api.types.is_period_dtype(time_level.dtype):\n",
      "C:\\Users\\tansc\\AppData\\Local\\Temp\\ipykernel_12184\\2146223430.py:14: DeprecationWarning: is_period_dtype is deprecated and will be removed in a future version. Use `isinstance(dtype, pd.PeriodDtype)` instead\n",
      "  if pd.api.types.is_period_dtype(time_level.dtype):\n",
      "C:\\Users\\tansc\\AppData\\Local\\Temp\\ipykernel_12184\\2146223430.py:14: DeprecationWarning: is_period_dtype is deprecated and will be removed in a future version. Use `isinstance(dtype, pd.PeriodDtype)` instead\n",
      "  if pd.api.types.is_period_dtype(time_level.dtype):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "timestamp_prefix = datetime.now().strftime(\"%y%m%d_%H%M\")\n",
    "print(f\"Script started at: {datetime.now()}\")\n",
    "print(f\"Saving directory: '{SAVE_DIR}'\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --- 1. Load Pre-Split Data & Check Index ---\n",
    "print(\"\\n--- Loading Pre-Split Datasets & Checking Index ---\")\n",
    "loaded_data = {}\n",
    "all_data_loaded = True\n",
    "for name, path in DATA_PATHS.items():\n",
    "    try:\n",
    "        df = pd.read_parquet(path)\n",
    "        df = check_and_convert_index(df, name)\n",
    "        loaded_data[name] = df\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR loading or checking index for {name}: {e}\")\n",
    "        all_data_loaded = False; break\n",
    "if not all_data_loaded: exit()\n",
    "\n",
    "# Assign loaded data to variables\n",
    "X_train = loaded_data['X_train']\n",
    "y_train = loaded_data['y_train']\n",
    "X_val = loaded_data.get('X_val')\n",
    "y_val = loaded_data.get('y_val')\n",
    "X_test = loaded_data.get('X_test')\n",
    "y_test = loaded_data.get('y_test')\n",
    "\n",
    "\n",
    "# --- 2. Identify Target Variable Name ---\n",
    "print(\"\\n--- Identifying Target Variable ---\")\n",
    "target_variable = y_train.columns[0]\n",
    "print(f\"Target variable identified as: '{target_variable}'\")\n",
    "# feature_list is implicitly all columns in X_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4a3745",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "160639a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      " Model Training \n",
      "==============================\n",
      "\n",
      "--- Training Pooled OLS ---\n",
      "Pooled OLS model fitted using 384.0 observations.\n",
      "\n",
      "--- Training Bank Fixed Effects (FE) ---\n",
      "Bank Fixed Effects model fitted using 384 observations.\n",
      "\n",
      "--- Training Pooled Random Forest ---\n",
      "  Skipping tuning, using default parameters.\n",
      "Random Forest fitted with default parameters.\n",
      "\n",
      "  --- Random Forest Feature Importances (Top 15) ---\n",
      "roa_lag1                      0.728738\n",
      "trading_assets_ratio_lag1     0.065668\n",
      "equity_to_asset_ratio_lag1    0.056706\n",
      "log_total_assets_lag1         0.045614\n",
      "deposit_ratio_lag1            0.021562\n",
      "household_delinq_diff_lag1    0.013678\n",
      "household_delinq_lag1         0.012587\n",
      "unemployment_lag1             0.012303\n",
      "gdp_qoq_lag1                  0.008704\n",
      "loan_to_asset_ratio_lag1      0.007179\n",
      "vix_qoq_lag1                  0.006713\n",
      "corp_bond_spread_lag1         0.005967\n",
      "unemployment_diff_lag1        0.005606\n",
      "sp500_qoq_lag1                0.002684\n",
      "cons_sentiment_qoq_lag1       0.002520\n",
      "\n",
      "--- Training Pooled XGBoost ---\n",
      "  Preparing validation set...\n",
      "  Validation set prepared.\n",
      "  Skipping tuning, using default parameters.\n",
      "XGBoost fitted with default parameters.\n",
      "  Best iteration: 30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 4. Train Models ---\n",
    "print(\"\\n\" + \"=\"*30); print(\" Model Training \"); print(\"=\"*30)\n",
    "trained_models = {}\n",
    "best_params_all = {}\n",
    "training_columns = {} # Reintroduced to store columns used by models (esp. OLS/FE)\n",
    "\n",
    "tune_config_dict = { 'do_tuning': DO_TUNING, 'n_iter': N_ITER_RANDOM_SEARCH, 'cv_folds': CV_FOLDS, 'scoring_metric': SCORING_METRIC }\n",
    "y_train_series = y_train[target_variable] # Use Series for y\n",
    "\n",
    "if MODELS_TO_TRAIN.get('Pooled OLS'):\n",
    "    model_ols, cols_ols = train_pooled_ols(X_train, y_train_series)\n",
    "    trained_models['Pooled OLS'] = model_ols\n",
    "    training_columns['Pooled OLS'] = cols_ols # Store fitted columns (incl. const)\n",
    "    best_params_all['Pooled OLS'] = None\n",
    "\n",
    "if MODELS_TO_TRAIN.get('Bank FE'):\n",
    "    model_fe, cols_fe = train_bank_fe(X_train, y_train_series)\n",
    "    trained_models['Bank FE'] = model_fe\n",
    "    training_columns['Bank FE'] = cols_fe # Store effective feature columns\n",
    "    best_params_all['Bank FE'] = None\n",
    "\n",
    "if MODELS_TO_TRAIN.get('Random Forest'):\n",
    "    # RF needs original features; training function returns them\n",
    "    model_rf, params_rf, cols_rf = train_random_forest(X_train, y_train_series, ENTITY_ID_COL, tune_config_dict)\n",
    "    trained_models['Random Forest'] = model_rf\n",
    "    best_params_all['Random Forest'] = params_rf\n",
    "    training_columns['Random Forest'] = cols_rf # Store feature columns used\n",
    "\n",
    "if MODELS_TO_TRAIN.get('XGBoost'):\n",
    "    # XGB needs original features; training function returns them\n",
    "    model_xgb, params_xgb, cols_xgb = train_xgboost(\n",
    "        X_train, y_train_series, ENTITY_ID_COL, tune_config_dict,\n",
    "        X_val=X_val, y_val=(y_val[target_variable] if y_val is not None else None)\n",
    "    )\n",
    "    trained_models['XGBoost'] = model_xgb\n",
    "    best_params_all['XGBoost'] = params_xgb\n",
    "    training_columns['XGBoost'] = cols_xgb # Store feature columns used\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c81461f",
   "metadata": {},
   "source": [
    "## Predict and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cb0b58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      " Model Prediction & Evaluation \n",
      "==============================\n",
      "\n",
      "--- Evaluating Pooled OLS ---\n",
      "  Generating predictions for Pooled OLS...\n",
      "  Generated 384 non-NaN Pooled OLS predictions.\n",
      "  Evaluating Pooled OLS on train set...\n",
      "    R2: 0.6958, RMSE: 0.0027, MAE: 0.0011 (384 obs)\n",
      "  Generating predictions for Pooled OLS...\n",
      "  Generated 28 non-NaN Pooled OLS predictions.\n",
      "  Evaluating Pooled OLS on val set...\n",
      "    R2: -2.4483, RMSE: 0.0012, MAE: 0.0009 (28 obs)\n",
      "  Generating predictions for Pooled OLS...\n",
      "  Generated 28 non-NaN Pooled OLS predictions.\n",
      "  Evaluating Pooled OLS on test set...\n",
      "    R2: -4.1109, RMSE: 0.0016, MAE: 0.0013 (28 obs)\n",
      "\n",
      "--- Evaluating Bank FE ---\n",
      "  Generating predictions for Bank FE...\n",
      "  Generated 384 non-NaN Bank FE predictions.\n",
      "  Evaluating Bank FE on train set...\n",
      "    R2: -248.6858, RMSE: 0.0779, MAE: 0.0778 (384 obs)\n",
      "  Generating predictions for Bank FE...\n",
      "  Generated 28 non-NaN Bank FE predictions.\n",
      "  Evaluating Bank FE on val set...\n",
      "    R2: -15807.8788, RMSE: 0.0788, MAE: 0.0788 (28 obs)\n",
      "  Generating predictions for Bank FE...\n",
      "  Generated 28 non-NaN Bank FE predictions.\n",
      "  Evaluating Bank FE on test set...\n",
      "    R2: -13632.7493, RMSE: 0.0804, MAE: 0.0803 (28 obs)\n",
      "\n",
      "--- Evaluating Random Forest ---\n",
      "  Generating predictions for Random Forest...\n",
      "  Generated 384 non-NaN Random Forest predictions.\n",
      "  Evaluating Random Forest on train set...\n",
      "    R2: 0.2413, RMSE: 0.0043, MAE: 0.0009 (384 obs)\n",
      "  Generating predictions for Random Forest...\n",
      "  Generated 28 non-NaN Random Forest predictions.\n",
      "  Evaluating Random Forest on val set...\n",
      "    R2: -0.3811, RMSE: 0.0007, MAE: 0.0006 (28 obs)\n",
      "  Generating predictions for Random Forest...\n",
      "  Generated 28 non-NaN Random Forest predictions.\n",
      "  Evaluating Random Forest on test set...\n",
      "    R2: -1.5495, RMSE: 0.0011, MAE: 0.0009 (28 obs)\n",
      "\n",
      "--- Evaluating XGBoost ---\n",
      "  Generating predictions for XGBoost...\n",
      "  Generated 384 non-NaN XGBoost predictions.\n",
      "  Evaluating XGBoost on train set...\n",
      "    R2: 0.5196, RMSE: 0.0034, MAE: 0.0010 (384 obs)\n",
      "  Generating predictions for XGBoost...\n",
      "  Generated 28 non-NaN XGBoost predictions.\n",
      "  Evaluating XGBoost on val set...\n",
      "    R2: 0.3242, RMSE: 0.0005, MAE: 0.0004 (28 obs)\n",
      "  Generating predictions for XGBoost...\n",
      "  Generated 28 non-NaN XGBoost predictions.\n",
      "  Evaluating XGBoost on test set...\n",
      "    R2: -0.6739, RMSE: 0.0009, MAE: 0.0007 (28 obs)\n",
      "\n",
      "========================================\n",
      " Consolidated Evaluation Results \n",
      "========================================\n",
      "\n",
      "--- Performance Summary (Metrics by Dataset) ---\n",
      "               R2_train      R2_val     R2_test  RMSE_train  RMSE_val  \\\n",
      "Model                                                                   \n",
      "Bank FE       -248.6858 -15807.8788 -13632.7493      0.0779    0.0788   \n",
      "Pooled OLS       0.6958     -2.4483     -4.1109      0.0027    0.0012   \n",
      "Random Forest    0.2413     -0.3811     -1.5495      0.0043    0.0007   \n",
      "XGBoost          0.5196      0.3242     -0.6739      0.0034    0.0005   \n",
      "\n",
      "               RMSE_test  MAE_train  MAE_val  MAE_test  MSE_train  MSE_val  \\\n",
      "Model                                                                        \n",
      "Bank FE           0.0804     0.0778   0.0788    0.0803     0.0061   0.0062   \n",
      "Pooled OLS        0.0016     0.0011   0.0009    0.0013     0.0000   0.0000   \n",
      "Random Forest     0.0011     0.0009   0.0006    0.0009     0.0000   0.0000   \n",
      "XGBoost           0.0009     0.0010   0.0004    0.0007     0.0000   0.0000   \n",
      "\n",
      "               MSE_test  \n",
      "Model                    \n",
      "Bank FE          0.0065  \n",
      "Pooled OLS       0.0000  \n",
      "Random Forest    0.0000  \n",
      "XGBoost          0.0000  \n",
      "\n",
      "Evaluation summary saved to 'models\\250517_1817_evaluation_summary.csv'\n",
      "\n",
      "========================================\n",
      " Descriptive Statistics (Test Set) \n",
      "========================================\n",
      "\n",
      "--- Actual y_test Values ---\n",
      "count    28.000000\n",
      "mean      0.002693\n",
      "std       0.000701\n",
      "min       0.001370\n",
      "25%       0.002119\n",
      "50%       0.002671\n",
      "75%       0.003121\n",
      "max       0.004751\n",
      "\n",
      "--- Pooled OLS Predictions on Test Set ---\n",
      "  Generating predictions for Pooled OLS...\n",
      "  Generated 28 non-NaN Pooled OLS predictions.\n",
      "count    28.000000\n",
      "mean      0.001420\n",
      "std       0.001093\n",
      "min      -0.001471\n",
      "25%       0.000996\n",
      "50%       0.001538\n",
      "75%       0.002107\n",
      "max       0.003127\n",
      "\n",
      "--- Bank FE Predictions on Test Set ---\n",
      "  Generating predictions for Bank FE...\n",
      "  Generated 28 non-NaN Bank FE predictions.\n",
      "count    28.000000\n",
      "mean     -0.077646\n",
      "std       0.003123\n",
      "min      -0.081783\n",
      "25%      -0.080076\n",
      "50%      -0.078589\n",
      "75%      -0.075255\n",
      "max      -0.071313\n",
      "\n",
      "--- Random Forest Predictions on Test Set ---\n",
      "  Generating predictions for Random Forest...\n",
      "  Generated 28 non-NaN Random Forest predictions.\n",
      "count    28.000000\n",
      "mean      0.001896\n",
      "std       0.001005\n",
      "min      -0.000120\n",
      "25%       0.001309\n",
      "50%       0.001666\n",
      "75%       0.002746\n",
      "max       0.003856\n",
      "\n",
      "--- XGBoost Predictions on Test Set ---\n",
      "  Generating predictions for XGBoost...\n",
      "  Generated 28 non-NaN XGBoost predictions.\n",
      "count    28.000000\n",
      "mean      0.002022\n",
      "std       0.000388\n",
      "min       0.000988\n",
      "25%       0.001908\n",
      "50%       0.002138\n",
      "75%       0.002229\n",
      "max       0.002519\n",
      "\n",
      "========================================\n",
      " Saving Models and Artifacts \n",
      "========================================\n",
      "\n",
      "--- Saving Artifacts for Pooled OLS ---\n",
      "  Saved model to 'models\\250517_1817_pooled_ols_model.joblib'\n",
      "\n",
      "--- Saving Artifacts for Bank FE ---\n",
      "  Saved model to 'models\\250517_1817_bank_fe_model.joblib'\n",
      "\n",
      "--- Saving Artifacts for Random Forest ---\n",
      "  Saved model to 'models\\250517_1817_random_forest_model.joblib'\n",
      "\n",
      "--- Saving Artifacts for XGBoost ---\n",
      "  Saved model to 'models\\250517_1817_xgboost_model.joblib'\n",
      "\n",
      "Successfully saved artifacts for 4 model(s).\n",
      "\n",
      "========================================\n",
      "Script finished at: 2025-05-17 18:17:52.001582\n",
      "Total execution time: 1.68 seconds.\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 5. Prediction and Evaluation ---\n",
    "print(\"\\n\" + \"=\"*30); print(\" Model Prediction & Evaluation \"); print(\"=\"*30)\n",
    "evaluation_results = {}\n",
    "models_to_evaluate = [m for m, should_train in MODELS_TO_TRAIN.items() if should_train and trained_models.get(m) is not None]\n",
    "\n",
    "evaluation_sets = {\n",
    "    'train': (X_train, y_train_series),\n",
    "    'val': (X_val, y_val[target_variable] if y_val is not None else None),\n",
    "    'test': (X_test, y_test[target_variable] if y_test is not None else None)\n",
    "}\n",
    "\n",
    "for model_name in models_to_evaluate:\n",
    "    model_obj = trained_models[model_name]\n",
    "    pred_cols = training_columns.get(model_name) # Get columns model expects/used\n",
    "    print(f\"\\n--- Evaluating {model_name} ---\")\n",
    "    evaluation_results[model_name] = {}\n",
    "    for data_name, (X_data, y_actual) in evaluation_sets.items():\n",
    "        if X_data is None or y_actual is None:\n",
    "            print(f\"  Skipping {data_name} set for {model_name}: Data missing.\")\n",
    "            evaluation_results[model_name][data_name] = {'R2': np.nan, 'MSE': np.nan, 'RMSE': np.nan, 'MAE': np.nan}\n",
    "            continue\n",
    "        # Generate predictions passing the required train_cols for OLS/FE\n",
    "        predictions = run_predictions(model_obj, model_name, X_data, train_cols=pred_cols)\n",
    "        # Evaluate\n",
    "        metrics = evaluate_predictions(y_actual, predictions, model_name, data_name)\n",
    "        evaluation_results[model_name][data_name] = metrics\n",
    "\n",
    "# --- 6. Consolidate and Display Results ---\n",
    "results_table = display_results(evaluation_results)\n",
    "if results_table is not None:\n",
    "    results_filename = os.path.join(SAVE_DIR, f\"{timestamp_prefix}_evaluation_summary.csv\")\n",
    "    try: results_table.to_csv(results_filename); print(f\"\\nEvaluation summary saved to '{results_filename}'\")\n",
    "    except Exception as save_e: print(f\"\\nWarning: Could not save summary CSV: {save_e}\")\n",
    "\n",
    "# --- 7. Optional: Descriptive Statistics (Test Set) ---\n",
    "print(\"\\n\" + \"=\"*40); print(\" Descriptive Statistics (Test Set) \"); print(\"=\"*40)\n",
    "y_test_series = y_test[target_variable] if y_test is not None else None\n",
    "if X_test is not None and y_test_series is not None:\n",
    "    print(\"\\n--- Actual y_test Values ---\")\n",
    "    print(y_test_series.describe().to_string())\n",
    "    for model_name in models_to_evaluate:\n",
    "         model_obj = trained_models.get(model_name)\n",
    "         if model_obj is None: continue\n",
    "         print(f\"\\n--- {model_name} Predictions on Test Set ---\")\n",
    "         pred_cols = training_columns.get(model_name) # Get necessary cols for prediction\n",
    "         predictions_test = run_predictions(model_obj, model_name, X_test, train_cols=pred_cols)\n",
    "         if predictions_test is not None:\n",
    "              predictions_aligned, _ = predictions_test.align(y_test_series, join='right')\n",
    "              # Only describe non-NaN predictions\n",
    "              print(predictions_aligned.dropna().describe().to_string())\n",
    "         else: print(\"  Could not generate predictions for description.\")\n",
    "else: print(\"Test data unavailable, skipping descriptive statistics.\")\n",
    "\n",
    "# --- 8. Save models and best parameters ---\n",
    "print(\"\\n\" + \"=\"*40); print(\" Saving Models and Artifacts \"); print(\"=\"*40)\n",
    "saved_count = 0\n",
    "for model_name in models_to_evaluate:\n",
    "    model_obj = trained_models.get(model_name)\n",
    "    if model_obj is not None:\n",
    "        save_model_artifacts(model_obj, model_name, best_params_all.get(model_name), SAVE_DIR, timestamp_prefix)\n",
    "        saved_count += 1\n",
    "if saved_count == 0: print(\"\\nNo models were successfully trained and saved.\")\n",
    "else: print(f\"\\nSuccessfully saved artifacts for {saved_count} model(s).\")\n",
    "\n",
    "# --- End ---\n",
    "end_time_float = time.time()\n",
    "print(\"\\n\" + \"=\"*40); print(f\"Script finished at: {datetime.now()}\");\n",
    "print(f\"Total execution time: {end_time_float - start_time:.2f} seconds.\"); print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd8bba4",
   "metadata": {},
   "source": [
    "# Investigate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94f56a1",
   "metadata": {},
   "source": [
    "## OLS model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03eda452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>roa</td>       <th>  R-squared:         </th> <td>   0.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.682</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   49.25</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 17 May 2025</td> <th>  Prob (F-statistic):</th> <td>1.38e-83</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:17:52</td>     <th>  Log-Likelihood:    </th> <td>  1723.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   384</td>      <th>  AIC:               </th> <td>  -3411.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   366</td>      <th>  BIC:               </th> <td>  -3340.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    17</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                      <td>   -0.0017</td> <td>    0.005</td> <td>   -0.331</td> <td> 0.741</td> <td>   -0.012</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gdp_qoq_lag1</th>               <td>    0.0543</td> <td>    0.031</td> <td>    1.753</td> <td> 0.080</td> <td>   -0.007</td> <td>    0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cpi_qoq_lag1</th>               <td>    0.0750</td> <td>    0.046</td> <td>    1.649</td> <td> 0.100</td> <td>   -0.014</td> <td>    0.165</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sp500_qoq_lag1</th>             <td>    0.0079</td> <td>    0.003</td> <td>    2.417</td> <td> 0.016</td> <td>    0.001</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>corp_bond_spread_lag1</th>      <td>    0.0539</td> <td>    0.052</td> <td>    1.042</td> <td> 0.298</td> <td>   -0.048</td> <td>    0.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cons_sentiment_qoq_lag1</th>    <td>    0.0048</td> <td>    0.003</td> <td>    1.603</td> <td> 0.110</td> <td>   -0.001</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>unemployment_lag1</th>          <td>   -0.0235</td> <td>    0.021</td> <td>   -1.133</td> <td> 0.258</td> <td>   -0.064</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>unemployment_diff_lag1</th>     <td>    0.0918</td> <td>    0.044</td> <td>    2.070</td> <td> 0.039</td> <td>    0.005</td> <td>    0.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>household_delinq_lag1</th>      <td>   -0.0299</td> <td>    0.018</td> <td>   -1.676</td> <td> 0.095</td> <td>   -0.065</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>household_delinq_diff_lag1</th> <td>   -0.1440</td> <td>    0.133</td> <td>   -1.085</td> <td> 0.279</td> <td>   -0.405</td> <td>    0.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vix_qoq_lag1</th>               <td>    0.0021</td> <td>    0.001</td> <td>    2.419</td> <td> 0.016</td> <td>    0.000</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>spread_10y_3m_lag1</th>         <td>    0.0390</td> <td>    0.028</td> <td>    1.414</td> <td> 0.158</td> <td>   -0.015</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_total_assets_lag1</th>      <td>    0.0001</td> <td>    0.000</td> <td>    0.621</td> <td> 0.535</td> <td>   -0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>deposit_ratio_lag1</th>         <td>   -0.0040</td> <td>    0.002</td> <td>   -2.583</td> <td> 0.010</td> <td>   -0.007</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>loan_to_asset_ratio_lag1</th>   <td>    0.0048</td> <td>    0.002</td> <td>    2.262</td> <td> 0.024</td> <td>    0.001</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>equity_to_asset_ratio_lag1</th> <td>   -0.0059</td> <td>    0.007</td> <td>   -0.832</td> <td> 0.406</td> <td>   -0.020</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>trading_assets_ratio_lag1</th>  <td>   -0.0041</td> <td>    0.006</td> <td>   -0.674</td> <td> 0.501</td> <td>   -0.016</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>roa_lag1</th>                   <td>    0.5354</td> <td>    0.023</td> <td>   23.511</td> <td> 0.000</td> <td>    0.491</td> <td>    0.580</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>343.621</td> <th>  Durbin-Watson:     </th>  <td>   2.674</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>163666.178</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.656</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>104.000</td> <th>  Cond. No.          </th>  <td>2.02e+04</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.02e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                 &       roa        & \\textbf{  R-squared:         } &     0.696   \\\\\n",
       "\\textbf{Model:}                         &       OLS        & \\textbf{  Adj. R-squared:    } &     0.682   \\\\\n",
       "\\textbf{Method:}                        &  Least Squares   & \\textbf{  F-statistic:       } &     49.25   \\\\\n",
       "\\textbf{Date:}                          & Sat, 17 May 2025 & \\textbf{  Prob (F-statistic):} &  1.38e-83   \\\\\n",
       "\\textbf{Time:}                          &     18:17:52     & \\textbf{  Log-Likelihood:    } &    1723.6   \\\\\n",
       "\\textbf{No. Observations:}              &         384      & \\textbf{  AIC:               } &    -3411.   \\\\\n",
       "\\textbf{Df Residuals:}                  &         366      & \\textbf{  BIC:               } &    -3340.   \\\\\n",
       "\\textbf{Df Model:}                      &          17      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}               &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                        & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}                          &      -0.0017  &        0.005     &    -0.331  &         0.741        &       -0.012    &        0.009     \\\\\n",
       "\\textbf{gdp\\_qoq\\_lag1}                 &       0.0543  &        0.031     &     1.753  &         0.080        &       -0.007    &        0.115     \\\\\n",
       "\\textbf{cpi\\_qoq\\_lag1}                 &       0.0750  &        0.046     &     1.649  &         0.100        &       -0.014    &        0.165     \\\\\n",
       "\\textbf{sp500\\_qoq\\_lag1}               &       0.0079  &        0.003     &     2.417  &         0.016        &        0.001    &        0.014     \\\\\n",
       "\\textbf{corp\\_bond\\_spread\\_lag1}       &       0.0539  &        0.052     &     1.042  &         0.298        &       -0.048    &        0.155     \\\\\n",
       "\\textbf{cons\\_sentiment\\_qoq\\_lag1}     &       0.0048  &        0.003     &     1.603  &         0.110        &       -0.001    &        0.011     \\\\\n",
       "\\textbf{unemployment\\_lag1}             &      -0.0235  &        0.021     &    -1.133  &         0.258        &       -0.064    &        0.017     \\\\\n",
       "\\textbf{unemployment\\_diff\\_lag1}       &       0.0918  &        0.044     &     2.070  &         0.039        &        0.005    &        0.179     \\\\\n",
       "\\textbf{household\\_delinq\\_lag1}        &      -0.0299  &        0.018     &    -1.676  &         0.095        &       -0.065    &        0.005     \\\\\n",
       "\\textbf{household\\_delinq\\_diff\\_lag1}  &      -0.1440  &        0.133     &    -1.085  &         0.279        &       -0.405    &        0.117     \\\\\n",
       "\\textbf{vix\\_qoq\\_lag1}                 &       0.0021  &        0.001     &     2.419  &         0.016        &        0.000    &        0.004     \\\\\n",
       "\\textbf{spread\\_10y\\_3m\\_lag1}          &       0.0390  &        0.028     &     1.414  &         0.158        &       -0.015    &        0.093     \\\\\n",
       "\\textbf{log\\_total\\_assets\\_lag1}       &       0.0001  &        0.000     &     0.621  &         0.535        &       -0.000    &        0.001     \\\\\n",
       "\\textbf{deposit\\_ratio\\_lag1}           &      -0.0040  &        0.002     &    -2.583  &         0.010        &       -0.007    &       -0.001     \\\\\n",
       "\\textbf{loan\\_to\\_asset\\_ratio\\_lag1}   &       0.0048  &        0.002     &     2.262  &         0.024        &        0.001    &        0.009     \\\\\n",
       "\\textbf{equity\\_to\\_asset\\_ratio\\_lag1} &      -0.0059  &        0.007     &    -0.832  &         0.406        &       -0.020    &        0.008     \\\\\n",
       "\\textbf{trading\\_assets\\_ratio\\_lag1}   &      -0.0041  &        0.006     &    -0.674  &         0.501        &       -0.016    &        0.008     \\\\\n",
       "\\textbf{roa\\_lag1}                      &       0.5354  &        0.023     &    23.511  &         0.000        &        0.491    &        0.580     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 343.621 & \\textbf{  Durbin-Watson:     } &     2.674   \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 163666.178  \\\\\n",
       "\\textbf{Skew:}          &   2.656 & \\textbf{  Prob(JB):          } &      0.00   \\\\\n",
       "\\textbf{Kurtosis:}      & 104.000 & \\textbf{  Cond. No.          } &  2.02e+04   \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 2.02e+04. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    roa   R-squared:                       0.696\n",
       "Model:                            OLS   Adj. R-squared:                  0.682\n",
       "Method:                 Least Squares   F-statistic:                     49.25\n",
       "Date:                Sat, 17 May 2025   Prob (F-statistic):           1.38e-83\n",
       "Time:                        18:17:52   Log-Likelihood:                 1723.6\n",
       "No. Observations:                 384   AIC:                            -3411.\n",
       "Df Residuals:                     366   BIC:                            -3340.\n",
       "Df Model:                          17                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================================\n",
       "                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------\n",
       "const                         -0.0017      0.005     -0.331      0.741      -0.012       0.009\n",
       "gdp_qoq_lag1                   0.0543      0.031      1.753      0.080      -0.007       0.115\n",
       "cpi_qoq_lag1                   0.0750      0.046      1.649      0.100      -0.014       0.165\n",
       "sp500_qoq_lag1                 0.0079      0.003      2.417      0.016       0.001       0.014\n",
       "corp_bond_spread_lag1          0.0539      0.052      1.042      0.298      -0.048       0.155\n",
       "cons_sentiment_qoq_lag1        0.0048      0.003      1.603      0.110      -0.001       0.011\n",
       "unemployment_lag1             -0.0235      0.021     -1.133      0.258      -0.064       0.017\n",
       "unemployment_diff_lag1         0.0918      0.044      2.070      0.039       0.005       0.179\n",
       "household_delinq_lag1         -0.0299      0.018     -1.676      0.095      -0.065       0.005\n",
       "household_delinq_diff_lag1    -0.1440      0.133     -1.085      0.279      -0.405       0.117\n",
       "vix_qoq_lag1                   0.0021      0.001      2.419      0.016       0.000       0.004\n",
       "spread_10y_3m_lag1             0.0390      0.028      1.414      0.158      -0.015       0.093\n",
       "log_total_assets_lag1          0.0001      0.000      0.621      0.535      -0.000       0.001\n",
       "deposit_ratio_lag1            -0.0040      0.002     -2.583      0.010      -0.007      -0.001\n",
       "loan_to_asset_ratio_lag1       0.0048      0.002      2.262      0.024       0.001       0.009\n",
       "equity_to_asset_ratio_lag1    -0.0059      0.007     -0.832      0.406      -0.020       0.008\n",
       "trading_assets_ratio_lag1     -0.0041      0.006     -0.674      0.501      -0.016       0.008\n",
       "roa_lag1                       0.5354      0.023     23.511      0.000       0.491       0.580\n",
       "==============================================================================\n",
       "Omnibus:                      343.621   Durbin-Watson:                   2.674\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           163666.178\n",
       "Skew:                           2.656   Prob(JB):                         0.00\n",
       "Kurtosis:                     104.000   Cond. No.                     2.02e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.02e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_model = trained_models.get('Pooled OLS')\n",
    "\n",
    "ols_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b606a9fd",
   "metadata": {},
   "source": [
    "## Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f588994",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "    # Use the design matrix from OLS training (X_train_ols from train_pooled_ols function, ensure it includes constant)\n",
    "    # Make sure X_train_ols exists from your training step or recalculate it\n",
    "    # X_train_ols_for_vif = sm.add_constant(X_train, has_constant='add') # Or use the one saved if possible\n",
    "\n",
    "    ols_fitted_cols = ols_model.model.exog_names # Get columns actually used in fit\n",
    "    X_train_ols_fitted = ols_model.model.exog # Get the design matrix used in fit\n",
    "\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = ols_fitted_cols\n",
    "    # Calculate VIF - requires the exog matrix used in the fit\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X_train_ols_fitted, i) for i in range(X_train_ols_fitted.shape[1])]\n",
    "\n",
    "    print(\"\\nVariance Inflation Factors (VIF):\")\n",
    "    print(vif_data.sort_values('VIF', ascending=False))\n",
    "    # Look for VIFs > 5 or 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b71db8",
   "metadata": {},
   "source": [
    "# Feature importance   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5172179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random Forest Feature Importances (Top 15) ---\n",
      "roa_lag1                      0.728738\n",
      "trading_assets_ratio_lag1     0.065668\n",
      "equity_to_asset_ratio_lag1    0.056706\n",
      "log_total_assets_lag1         0.045614\n",
      "deposit_ratio_lag1            0.021562\n",
      "household_delinq_diff_lag1    0.013678\n",
      "household_delinq_lag1         0.012587\n",
      "unemployment_lag1             0.012303\n",
      "gdp_qoq_lag1                  0.008704\n",
      "loan_to_asset_ratio_lag1      0.007179\n",
      "vix_qoq_lag1                  0.006713\n",
      "corp_bond_spread_lag1         0.005967\n",
      "unemployment_diff_lag1        0.005606\n",
      "sp500_qoq_lag1                0.002684\n",
      "cons_sentiment_qoq_lag1       0.002520\n",
      "\n",
      "--- XGBoost Feature Importances (Top 15) ---\n",
      "deposit_ratio_lag1            0.466591\n",
      "gdp_qoq_lag1                  0.165454\n",
      "equity_to_asset_ratio_lag1    0.097067\n",
      "roa_lag1                      0.080115\n",
      "cpi_qoq_lag1                  0.049659\n",
      "sp500_qoq_lag1                0.037519\n",
      "trading_assets_ratio_lag1     0.023894\n",
      "unemployment_diff_lag1        0.016345\n",
      "log_total_assets_lag1         0.009572\n",
      "household_delinq_lag1         0.008525\n",
      "loan_to_asset_ratio_lag1      0.008028\n",
      "vix_qoq_lag1                  0.008004\n",
      "unemployment_lag1             0.007069\n",
      "household_delinq_diff_lag1    0.006983\n",
      "cons_sentiment_qoq_lag1       0.005768\n"
     ]
    }
   ],
   "source": [
    "# For xgboost and rf, show feature importances\n",
    "if 'Random Forest' in trained_models:\n",
    "    rf_model = trained_models['Random Forest']\n",
    "    if hasattr(rf_model, 'feature_importances_'):\n",
    "        importances_rf = pd.Series(rf_model.feature_importances_, index=X_train.columns)\n",
    "        print(\"\\n--- Random Forest Feature Importances (Top 15) ---\")\n",
    "        print(importances_rf.nlargest(15).to_string())\n",
    "if 'XGBoost' in trained_models:\n",
    "    xgb_model = trained_models['XGBoost']\n",
    "    if hasattr(xgb_model, 'feature_importances_'):\n",
    "        importances_xgb = pd.Series(xgb_model.feature_importances_, index=X_train.columns)\n",
    "        print(\"\\n--- XGBoost Feature Importances (Top 15) ---\")\n",
    "        print(importances_xgb.nlargest(15).to_string())\n",
    "# Note: Feature importances for OLS/FE are not directly available, but can be inferred from coefficients\n",
    "# or VIF analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
